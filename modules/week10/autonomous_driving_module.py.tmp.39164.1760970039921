"""
Week 10: ììœ¨ì£¼í–‰ ì¸ì‹ ì‹œìŠ¤í…œ (End-to-End Autonomous Driving Pipeline)
"""

import streamlit as st
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from typing import List, Tuple, Optional, Dict, Any
import io
import os

from core.base_processor import BaseImageProcessor


class AutonomousDrivingModule(BaseImageProcessor):
    """Week 10: ììœ¨ì£¼í–‰ ì¸ì‹ ì‹œìŠ¤í…œ ëª¨ë“ˆ"""

    def __init__(self):
        super().__init__()
        self.name = "Week 10: Autonomous Driving Pipeline"

    def render(self):
        """ë©”ì¸ ë Œë”ë§ í•¨ìˆ˜"""
        st.title("ğŸš— Week 10: ììœ¨ì£¼í–‰ ì¸ì‹ ì‹œìŠ¤í…œ")

        st.markdown("""
        ## í•™ìŠµ ëª©í‘œ
        - **ì´ë¡ **: SAE ììœ¨ì£¼í–‰ ë ˆë²¨, ì„¼ì„œ ìœµí•©, ì¸ì‹-íŒë‹¨-ì œì–´ íŒŒì´í”„ë¼ì¸
        - **ì°¨ì„  ì¸ì‹**: ì§ì„ (Hough) â†’ ê³¡ì„ (Polynomial) â†’ ë”¥ëŸ¬ë‹(LaneNet)
        - **ê°ì²´ íƒì§€**: YOLOv8 + ByteTrack + IPM ê±°ë¦¬ ì¶”ì •
        - **í†µí•© ì‹œìŠ¤í…œ**: ìœ„í—˜ë„ ë¶„ì„ + ì˜ì‚¬ê²°ì • ë¡œì§
        - **3D ì‹œê°í™”**: BEV(Bird's Eye View) + 3D ë°”ìš´ë”© ë°•ìŠ¤
        - **ì‹¤ì „**: ì‹¤ì‹œê°„ ì¶”ë¡ , TensorRT ìµœì í™”, Edge ë°°í¬
        """)

        # í™˜ê²½ ì²´í¬
        self._check_environment()

        # 7ê°œ íƒ­ êµ¬ì„±
        tabs = st.tabs([
            "ğŸ“š ììœ¨ì£¼í–‰ ì´ë¡ ",
            "ğŸ›£ï¸ ì°¨ì„  ì¸ì‹ (3-Tier)",
            "ğŸš™ ê°ì²´ íƒì§€ ë° ì¶”ì ",
            "ğŸ”— í†µí•© íŒŒì´í”„ë¼ì¸",
            "ğŸ“ 3D ì‹œê°í™” (BEV)",
            "ğŸ® ê³ ê¸‰ ì‹œë®¬ë ˆì´í„°",
            "ğŸ’» ì‹¤ì „ ë°°í¬"
        ])

        with tabs[0]:
            self.render_theory()

        with tabs[1]:
            self.render_lane_detection()

        with tabs[2]:
            self.render_object_detection()

        with tabs[3]:
            self.render_integrated_pipeline()

        with tabs[4]:
            self.render_3d_visualization()

        with tabs[5]:
            self.render_simulator()

        with tabs[6]:
            self.render_deployment()

    def _check_environment(self):
        """í™˜ê²½ ì²´í¬ ë° ì„¤ì •"""
        with st.expander("ğŸ”§ í™˜ê²½ ì„¤ì • í™•ì¸", expanded=False):
            st.markdown("""
            ### í•„ìš”í•œ íŒ¨í‚¤ì§€
            - `opencv-python`: ì˜ìƒ ì²˜ë¦¬, ì°¨ì„  ì¸ì‹
            - `ultralytics`: YOLOv8 ê°ì²´ íƒì§€
            - `numpy`, `matplotlib`: ì‹œë®¬ë ˆì´ì…˜ ë° ì‹œê°í™”
            - `torch`: ë”¥ëŸ¬ë‹ ëª¨ë¸ (ì„ íƒì )

            ### 3-Tier ì‹¤í–‰ ì „ëµ
            1. **Full Mode**: ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ê¶Œì¥)
            2. **Basic Mode**: OpenCV + YOLOë§Œ ì‚¬ìš©
            3. **Simulation Mode**: ì‹œë®¬ë ˆì´ì…˜ë§Œ ì‹¤í–‰
            """)

            issues = []

            # Check opencv
            try:
                import cv2
                st.success(f"âœ… opencv-python {cv2.__version__}")
            except ImportError:
                issues.append("opencv-python")
                st.warning("âš ï¸ opencv-python ë¯¸ì„¤ì¹˜")

            # Check ultralytics
            try:
                import ultralytics
                st.success(f"âœ… ultralytics (YOLOv8)")
            except ImportError:
                issues.append("ultralytics")
                st.warning("âš ï¸ ultralytics ë¯¸ì„¤ì¹˜")

            # Check torch
            try:
                import torch
                device = "GPU" if torch.cuda.is_available() else "CPU"
                st.success(f"âœ… torch ({device})")
            except ImportError:
                issues.append("torch")
                st.info("â„¹ï¸ torch ë¯¸ì„¤ì¹˜ (ë”¥ëŸ¬ë‹ ê¸°ëŠ¥ ì œí•œ)")

            if issues:
                st.info(f"""
                ### ğŸ”§ ì„¤ì¹˜ ë°©ë²•
                ```bash
                pip install opencv-python ultralytics torch matplotlib numpy
                ```
                """)

    # ==================== Tab 1: ììœ¨ì£¼í–‰ ì´ë¡  ====================

    def render_theory(self):
        """ììœ¨ì£¼í–‰ ì´ë¡  ì„¤ëª…"""
        st.header("ğŸ“š ììœ¨ì£¼í–‰ ì´ë¡  ë° ì‹œìŠ¤í…œ êµ¬ì¡°")

        # 1. SAE ììœ¨ì£¼í–‰ ë ˆë²¨
        st.markdown("""
        ## 1. SAE ììœ¨ì£¼í–‰ ë ˆë²¨ (SAE J3016)

        SAE(Society of Automotive Engineers)ì—ì„œ ì •ì˜í•œ ììœ¨ì£¼í–‰ ë ˆë²¨ì€ ìë™í™” ìˆ˜ì¤€ì— ë”°ë¼ 0~5ë‹¨ê³„ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.
        """)

        level_cols = st.columns(6)
        levels = [
            ("ë ˆë²¨ 0", "ì™„ì „ ìˆ˜ë™", "ìš´ì „ìê°€ ëª¨ë“  ì œì–´", "âŒ", "#FF4444"),
            ("ë ˆë²¨ 1", "ìš´ì „ì ë³´ì¡°", "ACC, LKA ë“±", "âš¡", "#FF8844"),
            ("ë ˆë²¨ 2", "ë¶€ë¶„ ìë™í™”", "ì¡°í–¥+ê°€ê°ì†", "ğŸ”„", "#FFBB44"),
            ("ë ˆë²¨ 3", "ì¡°ê±´ë¶€ ìë™í™”", "íŠ¹ì • ì¡°ê±´ ììœ¨", "ğŸš—", "#88DD44"),
            ("ë ˆë²¨ 4", "ê³ ë„ ìë™í™”", "ëŒ€ë¶€ë¶„ ììœ¨", "ğŸ¤–", "#44BBFF"),
            ("ë ˆë²¨ 5", "ì™„ì „ ìë™í™”", "ëª¨ë“  ìƒí™© ììœ¨", "âœ¨", "#8844FF")
        ]

        for col, (level, name, desc, icon, color) in zip(level_cols, levels):
            with col:
                st.markdown(f"""
                <div style="background:{color}; padding:15px; border-radius:10px; text-align:center;">
                    <h1>{icon}</h1>
                    <h4>{level}</h4>
                    <p style="font-size:12px;"><b>{name}</b></p>
                    <p style="font-size:10px;">{desc}</p>
                </div>
                """, unsafe_allow_html=True)

        st.markdown("---")

        # 2. ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œ êµ¬ì¡°
        st.markdown("""
        ## 2. ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œ 3ë‹¨ê³„ êµ¬ì¡°

        ììœ¨ì£¼í–‰ì€ **ì¸ì‹(Perception) â†’ íŒë‹¨(Planning) â†’ ì œì–´(Control)** 3ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
        """)

        with st.expander("ğŸ“¸ 1ë‹¨ê³„: ì¸ì‹ (Perception)", expanded=True):
            st.markdown("""
            **ëª©ì **: ì£¼ë³€ í™˜ê²½ì„ ì´í•´í•˜ê³  ë””ì§€í„¸ ì •ë³´ë¡œ ë³€í™˜

            **ì„¼ì„œ ì¢…ë¥˜**:
            - **ì¹´ë©”ë¼**: ìƒ‰ìƒ, í˜•íƒœ, í…ìŠ¤íŠ¸ ì¸ì‹ (ì‹ í˜¸ë“±, í‘œì§€íŒ)
            - **ë¼ì´ë‹¤(LiDAR)**: 3D ê±°ë¦¬ ì¸¡ì •, ì •ë°€í•œ ë¬¼ì²´ ìœ„ì¹˜
            - **ë ˆì´ë”(Radar)**: ì¥ê±°ë¦¬ ì†ë„ ì¸¡ì •, ì•…ì²œí›„ ê°•ê±´
            - **ì´ˆìŒíŒŒ**: ê·¼ê±°ë¦¬ ì¥ì• ë¬¼ ê°ì§€ (ì£¼ì°¨ ë³´ì¡°)

            **ì¸ì‹ ê¸°ìˆ **:
            - ì°¨ì„  ì¸ì‹ (Lane Detection)
            - ê°ì²´ íƒì§€ (Object Detection): ì°¨ëŸ‰, ë³´í–‰ì, ì‹ í˜¸ë“±
            - ê°ì²´ ì¶”ì  (Object Tracking): ID ìœ ì§€
            - ê±°ë¦¬ ì¶”ì • (Depth Estimation)
            - ì„¸ê·¸ë©˜í…Œì´ì…˜ (Semantic/Instance Segmentation)

            **ì„¼ì„œ ìœµí•© (Sensor Fusion)**:
            - ì¹´ë©”ë¼ + ë¼ì´ë‹¤ â†’ ì •í™•í•œ 3D ìœ„ì¹˜
            - ë ˆì´ë” + ì¹´ë©”ë¼ â†’ ì•…ì²œí›„ ëŒ€ì‘
            - ì¹¼ë§Œ í•„í„°, ë² ì´ì§€ì•ˆ ìœµí•©
            """)

        with st.expander("ğŸ§  2ë‹¨ê³„: íŒë‹¨ (Planning)", expanded=True):
            st.markdown("""
            **ëª©ì **: ì¸ì‹ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì£¼í–‰ ì „ëµ ìˆ˜ë¦½

            **3-Level ê³„íš**:

            1. **Mission Planning (ë¯¸ì…˜ ê³„íš)**
               - ëª©ì ì§€ê¹Œì§€ ì „ì²´ ê²½ë¡œ ê³„íš
               - ê³ ì†ë„ë¡œ vs ì¼ë°˜ë„ë¡œ ì„ íƒ
               - íœ´ê²Œì†Œ, ì¶©ì „ì†Œ ê²½ìœ ì§€

            2. **Behavioral Planning (í–‰ë™ ê³„íš)**
               - ì°¨ì„  ë³€ê²½, ì¶”ì›”, íšŒì „ ê²°ì •
               - ì‹ í˜¸ë“± ëŒ€ê¸°, ë³´í–‰ì ì–‘ë³´
               - êµì°¨ë¡œ ì§„ì… íƒ€ì´ë°

            3. **Motion Planning (ë™ì‘ ê³„íš)**
               - ìµœì  ê¶¤ì (Trajectory) ìƒì„±
               - ê°€ì†/ê°ì† í”„ë¡œíŒŒì¼
               - ì¥ì• ë¬¼ íšŒí”¼ ê²½ë¡œ

            **ìœ„í—˜ë„ ë¶„ì„**:
            - TTC (Time To Collision): ì¶©ëŒê¹Œì§€ ë‚¨ì€ ì‹œê°„
            - ì°¨ì„  ì´íƒˆ ìœ„í—˜ë„
            - ì‚¬ê°ì§€ëŒ€ ê²½ê³ 

            **ì˜ì‚¬ê²°ì • ìš°ì„ ìˆœìœ„**:
            1. ì•ˆì „ (Safety First)
            2. ë²•ê·œ ì¤€ìˆ˜ (Traffic Rules)
            3. ìŠ¹ì°¨ê° (Comfort)
            4. íš¨ìœ¨ì„± (Efficiency)
            """)

        with st.expander("ğŸ® 3ë‹¨ê³„: ì œì–´ (Control)", expanded=True):
            st.markdown("""
            **ëª©ì **: ê³„íšëœ ê²½ë¡œë¥¼ ì •í™•íˆ ì¶”ì¢…í•˜ë„ë¡ ì°¨ëŸ‰ ì œì–´

            **ì œì–´ ì•Œê³ ë¦¬ì¦˜**:
            - **PID Controller**: ë¹„ë¡€-ì ë¶„-ë¯¸ë¶„ ì œì–´
            - **MPC (Model Predictive Control)**: ëª¨ë¸ ê¸°ë°˜ ì˜ˆì¸¡ ì œì–´
            - **Pure Pursuit**: ê²½ë¡œ ì¶”ì¢… ì•Œê³ ë¦¬ì¦˜
            - **Stanley Controller**: íš¡ë°©í–¥ ì œì–´

            **ì œì–´ ëŒ€ìƒ**:
            - ì¡°í–¥ê° (Steering Angle)
            - ê°€ì†/ê°ì† (Throttle/Brake)
            - ê¸°ì–´ ë³€ì†

            **í”¼ë“œë°± ë£¨í”„**:
            ```
            ê³„íš ê²½ë¡œ â†’ ì œì–´ê¸° â†’ ì•¡ì¶”ì—ì´í„° â†’ ì°¨ëŸ‰ ë™ì—­í•™
                â†‘                                      â†“
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¼ì„œ í”¼ë“œë°± â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            ```
            """)

        st.markdown("---")

        # 3. Week 10 íŒŒì´í”„ë¼ì¸
        st.markdown("""
        ## 3. Week 10 End-to-End íŒŒì´í”„ë¼ì¸

        ì´ë²ˆ ì£¼ì°¨ì—ì„œëŠ” **ì¸ì‹(Perception)** ë‹¨ê³„ë¥¼ ì§‘ì¤‘ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤.
        """)

        st.code("""
# Week 10 íŒŒì´í”„ë¼ì¸ êµ¬ì¡°

ì…ë ¥: ë„ë¡œ ì˜ìƒ (Video Stream)
    â†“
[1ë‹¨ê³„] ì°¨ì„  ì¸ì‹ (Lane Detection)
    â”œâ”€ Tier 1: Hough Transform (ì§ì„ )
    â”œâ”€ Tier 2: Polynomial Fitting (ê³¡ì„ )
    â””â”€ Tier 3: LaneNet (ë”¥ëŸ¬ë‹)
    â†“
[2ë‹¨ê³„] ê°ì²´ íƒì§€ (Object Detection)
    â”œâ”€ YOLOv8: ì°¨ëŸ‰/ë³´í–‰ì/ì‹ í˜¸ë“± íƒì§€
    â”œâ”€ ByteTrack: ID ìœ ì§€ ì¶”ì 
    â””â”€ IPM: ê±°ë¦¬ ì¶”ì •
    â†“
[3ë‹¨ê³„] ìœ„í—˜ë„ ë¶„ì„ (Risk Analysis)
    â”œâ”€ ì°¨ì„  ì´íƒˆ ìœ„í—˜ë„ (0~1)
    â”œâ”€ ì¶©ëŒ ìœ„í—˜ë„ (TTC)
    â””â”€ ê¸‰ì •ê±° ì°¨ëŸ‰ ê°ì§€
    â†“
[4ë‹¨ê³„] ì˜ì‚¬ê²°ì • (Decision Making)
    â”œâ”€ ì°¨ì„  ë³µê·€ (STEER_BACK)
    â”œâ”€ ê¸´ê¸‰ ì œë™ (EMERGENCY_BRAKE)
    â””â”€ ê°ì† (SLOW_DOWN)
    â†“
[5ë‹¨ê³„] ì‹œê°í™” (Visualization)
    â”œâ”€ 2D: ë°”ìš´ë”© ë°•ìŠ¤, ì°¨ì„  ì˜¤ë²„ë ˆì´
    â””â”€ 3D: BEV (Bird's Eye View)
        """, language='text')

        st.markdown("---")

        # 4. ì‹¤ì œ ì‚¬ë¡€
        st.markdown("""
        ## 4. ì‹¤ì œ ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œ ë¹„êµ
        """)

        comparison_data = {
            "ì‹œìŠ¤í…œ": ["Tesla Autopilot", "Waymo Driver", "GM Cruise", "í˜„ëŒ€ Highway"],
            "ë ˆë²¨": ["2-3", "4", "4", "2"],
            "ì„¼ì„œ": ["8 ì¹´ë©”ë¼", "ì¹´ë©”ë¼+ë¼ì´ë‹¤+ë ˆì´ë”", "ì¹´ë©”ë¼+ë¼ì´ë‹¤", "ì¹´ë©”ë¼+ë ˆì´ë”"],
            "ìš´í–‰ ì§€ì—­": ["ì „ì„¸ê³„", "ë¯¸êµ­ ì¼ë¶€", "ìƒŒí”„ë€ì‹œìŠ¤ì½”", "ê³ ì†ë„ë¡œ"],
            "íŠ¹ì§•": ["ì¹´ë©”ë¼ ì¤‘ì‹¬", "ë¼ì´ë‹¤ ì˜ì¡´", "ë„ì‹¬ ì£¼í–‰", "êµ­ë‚´ ìµœì´ˆ L3"]
        }

        import pandas as pd
        df = pd.DataFrame(comparison_data)
        st.dataframe(df, use_container_width=True)

        st.markdown("---")

        # 5. ê³¼ì œ
        st.markdown("""
        ## 5. í•™ìŠµ ê³¼ì œ

        **ì´ë¡  í•™ìŠµ**:
        - [ ] SAE ë ˆë²¨ë³„ ì°¨ì´ì  ì´í•´
        - [ ] ì„¼ì„œ ìœµí•©ì˜ í•„ìš”ì„± ì´í•´
        - [ ] ì¸ì‹-íŒë‹¨-ì œì–´ íŒŒì´í”„ë¼ì¸ ê·¸ë ¤ë³´ê¸°

        **ì‹¤ìŠµ ì¤€ë¹„**:
        - [ ] OpenCV, YOLO í™˜ê²½ ì„¤ì •
        - [ ] ìƒ˜í”Œ ë„ë¡œ ì˜ìƒ ì¤€ë¹„ (road_video.mp4)
        - [ ] ë‹¤ìŒ íƒ­ì—ì„œ ì°¨ì„  ì¸ì‹ ì‹¤ìŠµ ì§„í–‰
        """)

    # ==================== Tab 2: ì°¨ì„  ì¸ì‹ ====================

    def render_lane_detection(self):
        """ì°¨ì„  ì¸ì‹ 3-Tier êµ¬í˜„"""
        st.header("ğŸ›£ï¸ ì°¨ì„  ì¸ì‹ (Lane Detection)")

        st.markdown("""
        ## ì°¨ì„  ì¸ì‹ì˜ ì¤‘ìš”ì„±

        ì°¨ì„  ì¸ì‹ì€ ììœ¨ì£¼í–‰ì˜ **ê¸°ë³¸**ì´ì **í•µì‹¬**ì…ë‹ˆë‹¤:
        - ì°¨ëŸ‰ì˜ í˜„ì¬ ì°¨ì„  ìœ„ì¹˜ íŒŒì•…
        - ì°¨ì„  ì´íƒˆ ê²½ê³  (LDWS)
        - ì°¨ì„  ìœ ì§€ ë³´ì¡° (LKAS)
        - ì£¼í–‰ ê°€ëŠ¥ ì˜ì—­ ì •ì˜

        ### 3-Tier ì ‘ê·¼ë²•

        ë‚œì´ë„ì™€ ì •í™•ë„ì— ë”°ë¼ 3ê°€ì§€ ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.
        """)

        tier_tabs = st.tabs(["Tier 1: ì§ì„  ì°¨ì„ ", "Tier 2: ê³¡ì„  ì°¨ì„ ", "Tier 3: ë”¥ëŸ¬ë‹", "ğŸ“Š ë¹„êµ ë¶„ì„"])

        with tier_tabs[0]:
            self._render_lane_tier1()

        with tier_tabs[1]:
            self._render_lane_tier2()

        with tier_tabs[2]:
            self._render_lane_tier3()

        with tier_tabs[3]:
            self._render_lane_comparison()

    def _render_lane_tier1(self):
        """Tier 1: Hough Transform ì§ì„  ì°¨ì„ """
        st.subheader("Tier 1: Hough Transform (ì§ì„  ì°¨ì„ )")

        st.markdown("""
        **ê°œë…**: ì „í†µì ì¸ ì»´í“¨í„° ë¹„ì „ ê¸°ë²•ìœ¼ë¡œ ì§ì„  ì°¨ì„ ì„ ê²€ì¶œí•©ë‹ˆë‹¤.

        **ì¥ì **: ë¹ ë¦„, ê°„ë‹¨í•¨, ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥
        **ë‹¨ì **: ê³¡ì„  ì°¨ì„  ì²˜ë¦¬ ë¶ˆê°€, ì•…ì²œí›„ ì•½í•¨
        **ì ìš©**: ê³ ì†ë„ë¡œ ì§ì„  êµ¬ê°„
        """)

        # 5ë‹¨ê³„ íŒŒì´í”„ë¼ì¸
        pipeline_cols = st.columns(5)
        steps = [
            ("1ï¸âƒ£", "ì „ì²˜ë¦¬", "Gray + Blur"),
            ("2ï¸âƒ£", "ì—£ì§€ ê²€ì¶œ", "Canny Edge"),
            ("3ï¸âƒ£", "ROI ì„¤ì •", "ê´€ì‹¬ ì˜ì—­"),
            ("4ï¸âƒ£", "ì§ì„  ê²€ì¶œ", "Hough Transform"),
            ("5ï¸âƒ£", "ì‹œê°í™”", "Overlay")
        ]

        for col, (icon, title, desc) in zip(pipeline_cols, steps):
            with col:
                st.markdown(f"""
                <div style="background:#f0f2f6; padding:10px; border-radius:5px; text-align:center;">
                    <h2>{icon}</h2>
                    <h5>{title}</h5>
                    <p style="font-size:11px;">{desc}</p>
                </div>
                """, unsafe_allow_html=True)

        st.markdown("---")

        # ìƒì„¸ ì„¤ëª…
        with st.expander("ğŸ” ê° ë‹¨ê³„ ìƒì„¸ ì„¤ëª…", expanded=True):
            st.markdown("""
            ### 1ë‹¨ê³„: ì „ì²˜ë¦¬ (Preprocessing)

            ```python
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ (ë…¸ì´ì¦ˆ ì œê±°)
            blur = cv2.GaussianBlur(gray, (5, 5), 0)
            ```

            **ì´ìœ **: ìƒ‰ìƒ ì •ë³´ê°€ í•„ìš” ì—†ê³ , ë¸”ëŸ¬ë¡œ ë…¸ì´ì¦ˆë¥¼ ì¤„ì—¬ ì—£ì§€ ê²€ì¶œ ì •í™•ë„ í–¥ìƒ

            ---

            ### 2ë‹¨ê³„: ìºë‹ˆ ì—£ì§€ ê²€ì¶œ (Canny Edge Detection)

            ```python
            edges = cv2.Canny(blur, low_threshold=50, high_threshold=150)
            ```

            **ì›ë¦¬**:
            1. Gradient ê³„ì‚° (ë°ê¸° ë³€í™”)
            2. Non-Maximum Suppression (ê°€ì¥ ê°•í•œ ì—£ì§€ë§Œ)
            3. Double Threshold (ì•½í•œ/ê°•í•œ ì—£ì§€ êµ¬ë¶„)
            4. Edge Tracking (ì—°ê²°ëœ ì—£ì§€ë§Œ ìœ ì§€)

            **íŒŒë¼ë¯¸í„°**:
            - `low_threshold=50`: ì•½í•œ ì—£ì§€ ì„ê³„ê°’
            - `high_threshold=150`: ê°•í•œ ì—£ì§€ ì„ê³„ê°’

            ---

            ### 3ë‹¨ê³„: ROI (Region of Interest) ì„¤ì •

            ```python
            height, width = frame.shape[:2]

            # ì‚¼ê°í˜• ROI (ì°¨ì„ ì´ ìˆì„ ì˜ì—­)
            roi_vertices = np.array([[
                (0, height),                  # ì¢Œí•˜ë‹¨
                (width/2, height/2),          # ìƒë‹¨ ì¤‘ì•™
                (width, height)               # ìš°í•˜ë‹¨
            ]], dtype=np.int32)

            # ë§ˆìŠ¤í¬ ìƒì„±
            mask = np.zeros_like(edges)
            cv2.fillPoly(mask, roi_vertices, 255)

            # ë§ˆìŠ¤í¬ ì ìš©
            masked_edges = cv2.bitwise_and(edges, mask)
            ```

            **ì´ìœ **: í•˜ëŠ˜, ë‚˜ë¬´, ê°„íŒ ë“± ë¶ˆí•„ìš”í•œ ì—£ì§€ ì œê±° â†’ ì²˜ë¦¬ ì†ë„ í–¥ìƒ

            ---

            ### 4ë‹¨ê³„: Hough Transform ì§ì„  ê²€ì¶œ

            ```python
            lines = cv2.HoughLinesP(
                masked_edges,
                rho=2,              # ê±°ë¦¬ í•´ìƒë„ (í”½ì…€)
                theta=np.pi/180,    # ê°ë„ í•´ìƒë„ (1ë„)
                threshold=50,       # ìµœì†Œ êµì°¨ì  ìˆ˜
                minLineLength=40,   # ìµœì†Œ ì„  ê¸¸ì´
                maxLineGap=100      # ìµœëŒ€ ì„  ê°„ê²©
            )
            ```

            **Hough Transform ì›ë¦¬**:
            - ì´ë¯¸ì§€ ê³µê°„(x, y) â†’ Hough ê³µê°„(Ï, Î¸) ë³€í™˜
            - ì§ì„ : y = mx + c â†’ Ï = xÂ·cos(Î¸) + yÂ·sin(Î¸)
            - ë§ì€ ì ì´ êµì°¨í•˜ëŠ” (Ï, Î¸)ê°€ ì§ì„ 

            **íŒŒë¼ë¯¸í„° íŠœë‹**:
            - `threshold` â†‘ â†’ ê¸´ ì§ì„ ë§Œ ê²€ì¶œ
            - `minLineLength` â†‘ â†’ ì§§ì€ ì„  ì œê±°
            - `maxLineGap` â†‘ â†’ ëŠì–´ì§„ ì„  ì—°ê²°

            ---

            ### 5ë‹¨ê³„: ì‹œê°í™”

            ```python
            line_image = np.zeros_like(frame)

            if lines is not None:
                for line in lines:
                    x1, y1, x2, y2 = line[0]
                    cv2.line(line_image, (x1, y1), (x2, y2), (0, 0, 255), 2)

            # ë°˜íˆ¬ëª… í•©ì„±
            result = cv2.addWeighted(frame, 0.8, line_image, 1, 0)
            ```
            """)

        st.markdown("---")

        # ì „ì²´ ì½”ë“œ
        with st.expander("ğŸ“‹ ì „ì²´ Tier 1 ì½”ë“œ (Colab/ë¡œì»¬)", expanded=False):
            st.code("""
import cv2
import numpy as np

def detect_lanes_hough(frame):
    \"\"\"Tier 1: Hough Transform ì°¨ì„  ì¸ì‹\"\"\"

    # 1. ì „ì²˜ë¦¬
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)

    # 2. ìºë‹ˆ ì—£ì§€ ê²€ì¶œ
    edges = cv2.Canny(blur, 50, 150)

    # 3. ROI ì„¤ì •
    height, width = frame.shape[:2]
    roi_vertices = np.array([[
        (0, height),
        (width/2, height/2),
        (width, height)
    ]], dtype=np.int32)

    mask = np.zeros_like(edges)
    cv2.fillPoly(mask, roi_vertices, 255)
    masked_edges = cv2.bitwise_and(edges, mask)

    # 4. Hough Transform
    lines = cv2.HoughLinesP(
        masked_edges,
        rho=2,
        theta=np.pi/180,
        threshold=50,
        minLineLength=40,
        maxLineGap=100
    )

    # 5. ì‹œê°í™”
    line_image = np.zeros_like(frame)
    if lines is not None:
        for line in lines:
            x1, y1, x2, y2 = line[0]
            cv2.line(line_image, (x1, y1), (x2, y2), (0, 0, 255), 2)

    result = cv2.addWeighted(frame, 0.8, line_image, 1, 0)
    return result


def main():
    # ë¹„ë””ì˜¤ ë¡œë“œ
    cap = cv2.VideoCapture('road_video.mp4')  # ë˜ëŠ” 0 (ì›¹ìº )

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # ì°¨ì„  ì¸ì‹
        result = detect_lanes_hough(frame)

        # ê²°ê³¼ í‘œì‹œ
        cv2.imshow('Tier 1: Hough Transform', result)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
            """, language='python')

        st.markdown("---")

        # íŒŒë¼ë¯¸í„° ì‹¤í—˜
        st.markdown("### ğŸ§ª íŒŒë¼ë¯¸í„° ì‹¤í—˜ (ì‹œë®¬ë ˆì´ì…˜)")

        col1, col2 = st.columns(2)
        with col1:
            canny_low = st.slider("Canny Low Threshold", 0, 100, 50)
            canny_high = st.slider("Canny High Threshold", 100, 300, 150)
        with col2:
            hough_threshold = st.slider("Hough Threshold", 10, 100, 50)
            min_line_length = st.slider("Min Line Length", 10, 100, 40)

        st.info(f"""
        **í˜„ì¬ ì„¤ì •**:
        - Canny: [{canny_low}, {canny_high}]
        - Hough Threshold: {hough_threshold}
        - Min Line Length: {min_line_length}

        ğŸ’¡ **íŒ**: Cannyë¥¼ ë‚®ì¶”ë©´ ë” ë§ì€ ì—£ì§€, Houghë¥¼ ë†’ì´ë©´ ë” í™•ì‹¤í•œ ì§ì„ ë§Œ ê²€ì¶œ
        """)

    def _render_lane_tier2(self):
        """Tier 2: Polynomial Fitting ê³¡ì„  ì°¨ì„ """
        st.subheader("Tier 2: Polynomial Fitting (ê³¡ì„  ì°¨ì„ )")

        st.markdown("""
        **ê°œë…**: Sliding Windowì™€ ë‹¤í•­ì‹ í”¼íŒ…ìœ¼ë¡œ ê³¡ì„  ì°¨ì„ ì„ ê²€ì¶œí•©ë‹ˆë‹¤.

        **ì¥ì **: ê³¡ì„  ë„ë¡œ ì²˜ë¦¬ ê°€ëŠ¥, ì°¨ì„  ê³¡ë¥  ê³„ì‚° ê°€ëŠ¥
        **ë‹¨ì **: Tier 1ë³´ë‹¤ ëŠë¦¼, íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”
        **ì ìš©**: ì¼ë°˜ ë„ë¡œ, ì»¤ë¸Œ êµ¬ê°„
        """)

        # ì•Œê³ ë¦¬ì¦˜ ë‹¨ê³„
        st.markdown("### ì•Œê³ ë¦¬ì¦˜ ë‹¨ê³„")

        step_cols = st.columns(4)
        steps = [
            ("1ï¸âƒ£", "Perspective\nTransform", "BEV ë³€í™˜"),
            ("2ï¸âƒ£", "Histogram\nPeak", "ì°¨ì„  ì‹œì‘ì "),
            ("3ï¸âƒ£", "Sliding\nWindow", "ì°¨ì„  í”½ì…€ ì¶”ì¶œ"),
            ("4ï¸âƒ£", "Polynomial\nFit", "2ì°¨ ê³¡ì„  í”¼íŒ…")
        ]

        for col, (icon, title, desc) in zip(step_cols, steps):
            with col:
                st.markdown(f"""
                <div style="background:#e8f4f8; padding:12px; border-radius:5px; text-align:center;">
                    <h2>{icon}</h2>
                    <p style="font-size:13px; margin:0;"><b>{title}</b></p>
                    <p style="font-size:11px; color:#666;">{desc}</p>
                </div>
                """, unsafe_allow_html=True)

        st.markdown("---")

        with st.expander("ğŸ” Perspective Transform (BEV ë³€í™˜)", expanded=True):
            st.markdown("""
            **ëª©ì **: ì°¨ì„ ì„ ìœ„ì—ì„œ ë³¸ ê²ƒì²˜ëŸ¼ ë³€í™˜ (Bird's Eye View)

            ```python
            # Source points (ì›ë³¸ 4ì )
            src = np.float32([
                [width * 0.2, height],        # ì¢Œí•˜ë‹¨
                [width * 0.45, height * 0.6], # ì¢Œìƒë‹¨
                [width * 0.55, height * 0.6], # ìš°ìƒë‹¨
                [width * 0.8, height]         # ìš°í•˜ë‹¨
            ])

            # Destination points (ë³€í™˜ í›„ 4ì )
            dst = np.float32([
                [width * 0.2, height],        # ì¢Œí•˜ë‹¨
                [width * 0.2, 0],             # ì¢Œìƒë‹¨
                [width * 0.8, 0],             # ìš°ìƒë‹¨
                [width * 0.8, height]         # ìš°í•˜ë‹¨
            ])

            # ë³€í™˜ í–‰ë ¬ ê³„ì‚°
            M = cv2.getPerspectiveTransform(src, dst)

            # ë³€í™˜ ì ìš©
            warped = cv2.warpPerspective(edges, M, (width, height))
            ```

            **íš¨ê³¼**: í‰í–‰í•˜ì§€ ì•Šì€ ì°¨ì„ ì´ í‰í–‰í•˜ê²Œ ë³´ì„ â†’ ë‹¤í•­ì‹ í”¼íŒ… ìš©ì´
            """)

        with st.expander("ğŸ” Histogram & Sliding Window", expanded=True):
            st.markdown("""
            ### Histogramìœ¼ë¡œ ì°¨ì„  ì‹œì‘ì  ì°¾ê¸°

            ```python
            # ì´ë¯¸ì§€ í•˜ë‹¨ ì ˆë°˜ì˜ íˆìŠ¤í† ê·¸ë¨
            histogram = np.sum(warped[height//2:, :], axis=0)

            # ì¢Œìš° ì°¨ì„ ì˜ ì‹œì‘ì  (í”¼í¬ ìœ„ì¹˜)
            midpoint = len(histogram) // 2
            left_base = np.argmax(histogram[:midpoint])
            right_base = np.argmax(histogram[midpoint:]) + midpoint
            ```

            ### Sliding Windowë¡œ ì°¨ì„  í”½ì…€ ì¶”ì¶œ

            ```python
            # ìœˆë„ìš° ê°œìˆ˜ (9ê°œ)
            nwindows = 9
            window_height = height // nwindows

            # ì¢Œìš° ì°¨ì„  í”½ì…€ ì €ì¥
            left_lane_inds = []
            right_lane_inds = []

            # ê° ìœˆë„ìš°ë§ˆë‹¤ ë°˜ë³µ
            for window in range(nwindows):
                # ìœˆë„ìš° ê²½ê³„ ê³„ì‚°
                win_y_low = height - (window + 1) * window_height
                win_y_high = height - window * window_height

                # ì¢Œì¸¡ ìœˆë„ìš°
                win_xleft_low = left_current - margin
                win_xleft_high = left_current + margin

                # ìœˆë„ìš° ë‚´ í°ìƒ‰ í”½ì…€ ì°¾ê¸°
                good_left_inds = ((nonzeroy >= win_y_low) &
                                  (nonzeroy < win_y_high) &
                                  (nonzerox >= win_xleft_low) &
                                  (nonzerox < win_xleft_high)).nonzero()[0]

                left_lane_inds.append(good_left_inds)

                # ì¶©ë¶„í•œ í”½ì…€ì´ ìˆìœ¼ë©´ ì¤‘ì‹¬ ì—…ë°ì´íŠ¸
                if len(good_left_inds) > minpix:
                    left_current = int(np.mean(nonzerox[good_left_inds]))
            ```

            **ìœˆë„ìš° íŒŒë¼ë¯¸í„°**:
            - `nwindows=9`: ìœˆë„ìš° ê°œìˆ˜
            - `margin=100`: ìœˆë„ìš° í­ì˜ ì ˆë°˜
            - `minpix=50`: ì¤‘ì‹¬ ì—…ë°ì´íŠ¸ ìµœì†Œ í”½ì…€ ìˆ˜
            """)

        with st.expander("ğŸ” Polynomial Fitting (ë‹¤í•­ì‹ í”¼íŒ…)", expanded=True):
            st.markdown("""
            ### 2ì°¨ ë‹¤í•­ì‹ìœ¼ë¡œ ì°¨ì„  ê³¡ì„  í”¼íŒ…

            ```python
            # ì¶”ì¶œëœ ì°¨ì„  í”½ì…€
            leftx = nonzerox[left_lane_inds]
            lefty = nonzeroy[left_lane_inds]

            # 2ì°¨ ë‹¤í•­ì‹ í”¼íŒ…: x = ayÂ² + by + c
            left_fit = np.polyfit(lefty, leftx, 2)

            # ê³¡ì„  ìƒì„±
            ploty = np.linspace(0, height-1, height)
            left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]
            ```

            **ì™œ 2ì°¨ì‹ì¸ê°€?**:
            - ëŒ€ë¶€ë¶„ì˜ ë„ë¡œ ê³¡ì„ ì€ 2ì°¨ í•¨ìˆ˜ë¡œ ê·¼ì‚¬ ê°€ëŠ¥
            - ê³„ì‚° íš¨ìœ¨ì 
            - ê³¼ì í•©(Overfitting) ë°©ì§€

            ### ì°¨ì„  ê³¡ë¥  ê³„ì‚°

            ```python
            # ë¯¸í„° ë‹¨ìœ„ ë³€í™˜
            ym_per_pix = 30/720  # yì¶•: 30ë¯¸í„° / 720í”½ì…€
            xm_per_pix = 3.7/700 # xì¶•: 3.7ë¯¸í„° / 700í”½ì…€

            # ì‹¤ì œ ì¢Œí‘œë¡œ ë‹¤ì‹œ í”¼íŒ…
            left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)

            # ê³¡ë¥  ë°˜ì§€ë¦„ ê³„ì‚°
            y_eval = np.max(ploty)
            left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix +
                                   left_fit_cr[1])**2)**1.5) / np.abs(2*left_fit_cr[0])

            print(f"ì°¨ì„  ê³¡ë¥ : {left_curverad:.0f} ë¯¸í„°")
            ```

            **í™œìš©**:
            - ê³¡ë¥  â†’ ì¡°í–¥ê° ê³„ì‚°
            - ê¸‰ì»¤ë¸Œ ê²½ê³ 
            - ì†ë„ ì œí•œ ê¶Œì¥
            """)

        st.markdown("---")

        # ì „ì²´ ì½”ë“œ
        with st.expander("ğŸ“‹ ì „ì²´ Tier 2 ì½”ë“œ (Colab/ë¡œì»¬)", expanded=False):
            st.code("""
import cv2
import numpy as np

def detect_lanes_polynomial(frame):
    \"\"\"Tier 2: Polynomial Fitting ì°¨ì„  ì¸ì‹\"\"\"

    # 1. ì „ì²˜ë¦¬
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)

    # 2. Perspective Transform (BEV)
    height, width = frame.shape[:2]
    src = np.float32([
        [width * 0.2, height],
        [width * 0.45, height * 0.6],
        [width * 0.55, height * 0.6],
        [width * 0.8, height]
    ])
    dst = np.float32([
        [width * 0.2, height],
        [width * 0.2, 0],
        [width * 0.8, 0],
        [width * 0.8, height]
    ])
    M = cv2.getPerspectiveTransform(src, dst)
    warped = cv2.warpPerspective(edges, M, (width, height))

    # 3. Histogramìœ¼ë¡œ ì‹œì‘ì  ì°¾ê¸°
    histogram = np.sum(warped[height//2:, :], axis=0)
    midpoint = len(histogram) // 2
    left_base = np.argmax(histogram[:midpoint])
    right_base = np.argmax(histogram[midpoint:]) + midpoint

    # 4. Sliding Window
    nwindows = 9
    window_height = height // nwindows
    margin = 100
    minpix = 50

    nonzero = warped.nonzero()
    nonzeroy = np.array(nonzero[0])
    nonzerox = np.array(nonzero[1])

    left_current = left_base
    right_current = right_base

    left_lane_inds = []
    right_lane_inds = []

    for window in range(nwindows):
        win_y_low = height - (window + 1) * window_height
        win_y_high = height - window * window_height

        win_xleft_low = left_current - margin
        win_xleft_high = left_current + margin
        win_xright_low = right_current - margin
        win_xright_high = right_current + margin

        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) &
                          (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]
        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) &
                           (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]

        left_lane_inds.append(good_left_inds)
        right_lane_inds.append(good_right_inds)

        if len(good_left_inds) > minpix:
            left_current = int(np.mean(nonzerox[good_left_inds]))
        if len(good_right_inds) > minpix:
            right_current = int(np.mean(nonzerox[good_right_inds]))

    # 5. Polynomial Fitting
    left_lane_inds = np.concatenate(left_lane_inds)
    right_lane_inds = np.concatenate(right_lane_inds)

    leftx = nonzerox[left_lane_inds]
    lefty = nonzeroy[left_lane_inds]
    rightx = nonzerox[right_lane_inds]
    righty = nonzeroy[right_lane_inds]

    left_fit = np.polyfit(lefty, leftx, 2)
    right_fit = np.polyfit(righty, rightx, 2)

    # 6. ê³¡ì„  ìƒì„±
    ploty = np.linspace(0, height-1, height)
    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]
    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]

    # 7. ì‹œê°í™”
    warp_zero = np.zeros_like(warped).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))

    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])
    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])
    pts = np.hstack((pts_left, pts_right))

    cv2.fillPoly(color_warp, np.int_([pts]), (0, 255, 0))

    # Inverse Perspective Transform
    Minv = cv2.getPerspectiveTransform(dst, src)
    newwarp = cv2.warpPerspective(color_warp, Minv, (width, height))

    result = cv2.addWeighted(frame, 1, newwarp, 0.3, 0)

    return result


def main():
    cap = cv2.VideoCapture('road_video.mp4')

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        result = detect_lanes_polynomial(frame)
        cv2.imshow('Tier 2: Polynomial Fitting', result)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
            """, language='python')

    def _render_lane_tier3(self):
        """Tier 3: ë”¥ëŸ¬ë‹ ì°¨ì„  ì¸ì‹"""
        st.subheader("Tier 3: ë”¥ëŸ¬ë‹ (LaneNet, SCNN)")

        st.markdown("""
        **ê°œë…**: ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ ì°¨ì„ ì„ ì„¸ê·¸ë©˜í…Œì´ì…˜í•©ë‹ˆë‹¤.

        **ì¥ì **:
        - ë³µì¡í•œ í™˜ê²½ ëŒ€ì‘ (ì•…ì²œí›„, ì•¼ê°„, ê·¸ë¦¼ì)
        - ê³¡ì„ /ì§ì„  êµ¬ë¶„ ë¶ˆí•„ìš”
        - End-to-End í•™ìŠµ ê°€ëŠ¥

        **ë‹¨ì **:
        - ëŠë¦¼ (GPU í•„ìš”)
        - í•™ìŠµ ë°ì´í„° í•„ìš”
        - ëª¨ë¸ í¬ê¸° í¼

        **ì ìš©**: ëª¨ë“  ë„ë¡œ í™˜ê²½
        """)

        # ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¹„êµ
        st.markdown("### ì£¼ìš” ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¹„êµ")

        model_data = {
            "ëª¨ë¸": ["LaneNet", "SCNN", "Ultra-Fast-Lane", "PolyLaneNet"],
            "êµ¬ì¡°": ["Encoder-Decoder", "Slice CNN", "Row Anchor", "Polynomial"],
            "FPS": ["~30", "~15", "~320", "~80"],
            "ì •í™•ë„": ["ë†’ìŒ", "ë§¤ìš° ë†’ìŒ", "ì¤‘ê°„", "ë†’ìŒ"],
            "íŠ¹ì§•": ["Instance Seg", "Spatial Info", "ì´ˆê³ ì†", "ê³¡ì„  ì í•©"]
        }

        import pandas as pd
        df = pd.DataFrame(model_data)
        st.dataframe(df, use_container_width=True)

        st.markdown("---")

        with st.expander("ğŸ” LaneNet ì•„í‚¤í…ì²˜", expanded=True):
            st.markdown("""
            ### LaneNet êµ¬ì¡°

            ```
            ì…ë ¥ ì´ë¯¸ì§€ (H Ã— W Ã— 3)
                â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Encoder (ENet)              â”‚
            â”‚  - Feature Extraction       â”‚
            â”‚  - Downsampling             â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Binary Branch  â”‚ Embedding Br.  â”‚
            â”‚ (ì°¨ì„  ì—¬ë¶€)     â”‚ (ì°¨ì„  ID)      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“                   â†“
            Binary Seg Map    Instance Seg Map
            (H Ã— W Ã— 1)      (H Ã— W Ã— 4)
                â†“                   â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Post-Processing                  â”‚
            â”‚  - Clustering (DBSCAN)          â”‚
            â”‚  - Curve Fitting                â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
            ì°¨ì„  ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸
            ```

            ### Binary Segmentation Branch

            - **ëª©ì **: í”½ì…€ì´ ì°¨ì„ ì¸ì§€ ì•„ë‹Œì§€ ë¶„ë¥˜ (0 or 1)
            - **ì†ì‹¤ í•¨ìˆ˜**: Binary Cross-Entropy

            ### Embedding Branch

            - **ëª©ì **: ê° ì°¨ì„ ì— ê³ ìœ í•œ ID ë¶€ì—¬ (Instance Segmentation)
            - **ì†ì‹¤ í•¨ìˆ˜**: Discriminative Loss
              - ê°™ì€ ì°¨ì„  í”½ì…€ë¼ë¦¬ëŠ” ê°€ê¹ê²Œ
              - ë‹¤ë¥¸ ì°¨ì„  í”½ì…€ë¼ë¦¬ëŠ” ë©€ê²Œ

            ### Post-Processing

            1. **Clustering**: ê°™ì€ ì„ë² ë”© ê°’ì„ ê°€ì§„ í”½ì…€ì„ í•˜ë‚˜ì˜ ì°¨ì„ ìœ¼ë¡œ ê·¸ë£¹í™”
            2. **Curve Fitting**: ê° ì°¨ì„  í”½ì…€ì— ë‹¤í•­ì‹ í”¼íŒ…
            """)

        with st.expander("ğŸ” ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°", expanded=True):
            st.markdown("""
            ### HuggingFaceì—ì„œ ëª¨ë¸ ë¡œë“œ

            ```python
            # Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ Segformer ì‚¬ìš©
            from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation
            import torch
            from PIL import Image

            # ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ë¡œë“œ
            feature_extractor = SegformerFeatureExtractor.from_pretrained(
                "nvidia/segformer-b0-finetuned-ade-512-512"
            )
            model = SegformerForSemanticSegmentation.from_pretrained(
                "nvidia/segformer-b0-finetuned-ade-512-512"
            )

            # ì¶”ë¡ 
            def detect_lanes_dl(image):
                # ì „ì²˜ë¦¬
                inputs = feature_extractor(images=image, return_tensors="pt")

                # ëª¨ë¸ ì˜ˆì¸¡
                with torch.no_grad():
                    outputs = model(**inputs)

                # Logits â†’ Segmentation Map
                logits = outputs.logits
                seg_map = torch.argmax(logits, dim=1)[0]

                # ì°¨ì„  í´ë˜ìŠ¤ë§Œ ì¶”ì¶œ (í´ë˜ìŠ¤ ë²ˆí˜¸ëŠ” ë°ì´í„°ì…‹ ì˜ì¡´)
                lane_mask = (seg_map == LANE_CLASS_ID).numpy()

                return lane_mask
            ```

            ### TuSimple/CULane ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹

            ```python
            from torch.utils.data import DataLoader
            from transformers import Trainer, TrainingArguments

            # í•™ìŠµ ì„¤ì •
            training_args = TrainingArguments(
                output_dir="./lane_model",
                per_device_train_batch_size=4,
                num_train_epochs=50,
                learning_rate=5e-5,
                logging_steps=100,
                save_steps=500,
            )

            # Trainer
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
            )

            # í•™ìŠµ ì‹œì‘
            trainer.train()
            ```
            """)

        st.markdown("---")

        # ì „ì²´ ì½”ë“œ (ê°„ë‹¨ ë²„ì „)
        with st.expander("ğŸ“‹ Tier 3 ì½”ë“œ (ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ì‚¬ìš©)", expanded=False):
            st.code("""
# Tier 3: ë”¥ëŸ¬ë‹ ì°¨ì„  ì¸ì‹ (Segformer ì‚¬ìš©)

import cv2
import numpy as np
import torch
from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation
from PIL import Image

# ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ)
feature_extractor = SegformerFeatureExtractor.from_pretrained(
    "nvidia/segformer-b0-finetuned-ade-512-512"
)
model = SegformerForSemanticSegmentation.from_pretrained(
    "nvidia/segformer-b0-finetuned-ade-512-512"
)

# GPU ì‚¬ìš© (ê°€ëŠ¥í•˜ë©´)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)


def detect_lanes_dl(frame):
    \"\"\"ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì°¨ì„  ì¸ì‹\"\"\"

    # OpenCV (BGR) â†’ PIL (RGB)
    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

    # ì „ì²˜ë¦¬
    inputs = feature_extractor(images=image, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # ì¶”ë¡ 
    with torch.no_grad():
        outputs = model(**inputs)

    # Logits â†’ Segmentation Map
    logits = outputs.logits
    logits = torch.nn.functional.interpolate(
        logits,
        size=image.size[::-1],  # (height, width)
        mode="bilinear",
        align_corners=False
    )
    seg_map = torch.argmax(logits, dim=1)[0].cpu().numpy()

    # ì°¨ì„  í´ë˜ìŠ¤ ì¶”ì¶œ (ì˜ˆ: í´ë˜ìŠ¤ 6ì´ ë„ë¡œë¼ê³  ê°€ì •)
    # ì‹¤ì œë¡œëŠ” TuSimple/CULane íŒŒì¸íŠœë‹ í•„ìš”
    lane_mask = (seg_map == 6).astype(np.uint8) * 255

    # ìƒ‰ìƒ ì˜¤ë²„ë ˆì´
    lane_color = np.zeros_like(frame)
    lane_color[lane_mask > 0] = [0, 255, 0]  # ì´ˆë¡ìƒ‰

    result = cv2.addWeighted(frame, 0.7, lane_color, 0.3, 0)

    return result


def main():
    cap = cv2.VideoCapture('road_video.mp4')

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        result = detect_lanes_dl(frame)
        cv2.imshow('Tier 3: Deep Learning', result)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()


# ì°¸ê³ : ì‹¤ì œ ì°¨ì„  ì¸ì‹ì„ ìœ„í•´ì„œëŠ” TuSimple/CULane ë°ì´í„°ì…‹ìœ¼ë¡œ
# íŒŒì¸íŠœë‹í•œ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
# HuggingFace Hubì—ì„œ "lane-detection" ê²€ìƒ‰í•˜ì—¬ ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ì°¾ê¸°
            """, language='python')

    def _render_lane_comparison(self):
        """3-Tier ë¹„êµ ë¶„ì„"""
        st.subheader("ğŸ“Š 3-Tier ë¹„êµ ë¶„ì„")

        st.markdown("""
        ### ì„±ëŠ¥ ë¹„êµí‘œ
        """)

        comparison_data = {
            "ì§€í‘œ": ["ì²˜ë¦¬ ì†ë„ (FPS)", "ì •í™•ë„ (%)", "ê³¡ì„  ëŒ€ì‘", "ì•…ì²œí›„ ê°•ê±´ì„±", "GPU í•„ìš”", "êµ¬í˜„ ë‚œì´ë„", "ê¶Œì¥ í™˜ê²½"],
            "Tier 1 (Hough)": ["60-120", "75-85", "âŒ ë¶ˆê°€", "âš ï¸ ì•½í•¨", "âŒ", "â­", "ê³ ì†ë„ë¡œ ì§ì„ "],
            "Tier 2 (Polynomial)": ["30-60", "85-92", "âœ… ê°€ëŠ¥", "âš ï¸ ì•½í•¨", "âŒ", "â­â­â­", "ì¼ë°˜ ë„ë¡œ ê³¡ì„ "],
            "Tier 3 (Deep Learning)": ["10-30", "92-98", "âœ… ê°€ëŠ¥", "âœ… ê°•í•¨", "âœ…", "â­â­â­â­â­", "ëª¨ë“  í™˜ê²½"]
        }

        import pandas as pd
        df = pd.DataFrame(comparison_data)
        st.dataframe(df, use_container_width=True)

        st.markdown("---")

        # ì‹œê°ì  ë¹„êµ
        st.markdown("### ì‹œê°ì  ë¹„êµ")

        scenario_cols = st.columns(3)
        scenarios = [
            ("ğŸŒ ë§‘ì€ ë‚  ì§ì„ ", "Tier 1: â­â­â­â­â­\nTier 2: â­â­â­â­\nTier 3: â­â­â­â­â­"),
            ("ğŸŒ€ ê³¡ì„  ë„ë¡œ", "Tier 1: â­\nTier 2: â­â­â­â­\nTier 3: â­â­â­â­â­"),
            ("ğŸŒ§ï¸ ë¹„ì˜¤ëŠ” ë‚ ", "Tier 1: â­\nTier 2: â­â­\nTier 3: â­â­â­â­")
        ]

        for col, (title, ratings) in zip(scenario_cols, scenarios):
            with col:
                st.markdown(f"""
                <div style="background:#f8f9fa; padding:15px; border-radius:10px;">
                    <h4>{title}</h4>
                    <pre style="font-size:12px;">{ratings}</pre>
                </div>
                """, unsafe_allow_html=True)

        st.markdown("---")

        # ì„ íƒ ê°€ì´ë“œ
        st.markdown("""
        ### ğŸ¯ ì„ íƒ ê°€ì´ë“œ

        **Tier 1 (Hough Transform)ì„ ì„ íƒí•˜ì„¸ìš”:**
        - âœ… ê³ ì†ë„ë¡œ ì§ì„  êµ¬ê°„
        - âœ… ì‹¤ì‹œê°„ ì²˜ë¦¬ í•„ìˆ˜ (ì„ë² ë””ë“œ ì‹œìŠ¤í…œ)
        - âœ… ê°„ë‹¨í•œ í”„ë¡œí† íƒ€ì…
        - âœ… GPU ì—†ìŒ

        **Tier 2 (Polynomial)ë¥¼ ì„ íƒí•˜ì„¸ìš”:**
        - âœ… ì¼ë°˜ ë„ë¡œ (ê³¡ì„  í¬í•¨)
        - âœ… ì°¨ì„  ê³¡ë¥  ì •ë³´ í•„ìš”
        - âœ… ì¤‘ê°„ ì •í™•ë„ ìš”êµ¬
        - âœ… GPU ì—†ìŒ

        **Tier 3 (Deep Learning)ì„ ì„ íƒí•˜ì„¸ìš”:**
        - âœ… ë³µì¡í•œ í™˜ê²½ (ë„ì‹¬, ê³µì‚¬êµ¬ê°„)
        - âœ… ì•…ì²œí›„ ëŒ€ì‘ í•„ìˆ˜
        - âœ… ìµœê³  ì •í™•ë„ ìš”êµ¬
        - âœ… GPU ì‚¬ìš© ê°€ëŠ¥
        - âœ… í•™ìŠµ ë°ì´í„° í™•ë³´ ê°€ëŠ¥

        ---

        ### ğŸ’¡ ì‹¤ì „ íŒ: í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼

        ì‹¤ì œ ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œì€ ì—¬ëŸ¬ ë°©ë²•ì„ ì¡°í•©í•©ë‹ˆë‹¤:

        ```python
        def detect_lanes_hybrid(frame):
            # 1ì°¨: ë”¥ëŸ¬ë‹ (ì‹ ë¢°ë„ ë†’ìœ¼ë©´ ë°”ë¡œ ì‚¬ìš©)
            lane_dl, confidence = detect_lanes_dl(frame)
            if confidence > 0.9:
                return lane_dl

            # 2ì°¨: Polynomial Fallback
            lane_poly = detect_lanes_polynomial(frame)
            if is_valid(lane_poly):
                return lane_poly

            # 3ì°¨: Hough Fallback
            return detect_lanes_hough(frame)
        ```

        **ì¥ì **:
        - ì •ìƒ ìƒí™©: ë”¥ëŸ¬ë‹ì˜ ë†’ì€ ì •í™•ë„
        - ë¹„ì •ìƒ ìƒí™©: ì „í†µ ë°©ì‹ì˜ ì•ˆì •ì„±
        - ì‹¤íŒ¨ í™•ë¥  ìµœì†Œí™”
        """)

    # ==================== Tab 3: ê°ì²´ íƒì§€ ë° ì¶”ì  ====================

    def render_object_detection(self):
        """ê°ì²´ íƒì§€ ë° ì¶”ì  êµ¬í˜„"""
        st.header("ğŸš™ ê°ì²´ íƒì§€ ë° ì¶”ì ")

        st.markdown("""
        ## ê°ì²´ íƒì§€ì˜ ì¤‘ìš”ì„±

        ììœ¨ì£¼í–‰ì—ì„œ ì£¼ë³€ ê°ì²´ë¥¼ ì •í™•íˆ ì¸ì‹í•˜ëŠ” ê²ƒì€ ì•ˆì „ì˜ í•µì‹¬ì…ë‹ˆë‹¤:
        - ì°¨ëŸ‰: ì¶©ëŒ ë°©ì§€, ì°¨ê°„ ê±°ë¦¬ ìœ ì§€
        - ë³´í–‰ì: íš¡ë‹¨ ê°ì§€, ê¸‰ì •ê±°
        - ì‹ í˜¸ë“±/í‘œì§€íŒ: êµí†µ ê·œì¹™ ì¤€ìˆ˜
        - ê¸°íƒ€: ì´ë¥œì°¨, ë™ë¬¼, ë‚™í•˜ë¬¼
        """)

        obj_tabs = st.tabs([
            "ğŸ¯ YOLOv8 íƒì§€",
            "ğŸ”— ByteTrack ì¶”ì ",
            "ğŸ“ ê±°ë¦¬ ì¶”ì • (IPM)",
            "ğŸ“‹ ì „ì²´ ì½”ë“œ"
        ])

        with obj_tabs[0]:
            self._render_yolov8()

        with obj_tabs[1]:
            self._render_bytetrack()

        with obj_tabs[2]:
            self._render_ipm()

        with obj_tabs[3]:
            self._render_object_full_code()

    def _render_yolov8(self):
        """YOLOv8 ê°ì²´ íƒì§€"""
        st.subheader("ğŸ¯ YOLOv8 ê°ì²´ íƒì§€")

        st.markdown("""
        **YOLO (You Only Look Once)**: ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ì˜ ëŒ€í‘œ ì•Œê³ ë¦¬ì¦˜

        **YOLOv8 íŠ¹ì§•**:
        - Anchor-Free ë””ìì¸
        - ë¹ ë¥¸ ì†ë„ (V100 GPUì—ì„œ ~200 FPS)
        - ë†’ì€ ì •í™•ë„ (COCO mAP 53.9%)
        - ë‹¤ì–‘í•œ í¬ê¸° (n/s/m/l/x)
        """)

        # YOLOv8 ëª¨ë¸ í¬ê¸° ë¹„êµ
        model_cols = st.columns(5)
        models = [
            ("YOLOv8n", "Nano", "3.2M", "~300 FPS", "ì„ë² ë””ë“œ"),
            ("YOLOv8s", "Small", "11.2M", "~200 FPS", "ì—£ì§€"),
            ("YOLOv8m", "Medium", "25.9M", "~150 FPS", "ê¶Œì¥"),
            ("YOLOv8l", "Large", "43.7M", "~100 FPS", "ê³ ì„±ëŠ¥"),
            ("YOLOv8x", "X-Large", "68.2M", "~80 FPS", "ìµœê³ ì •í™•ë„")
        ]

        for col, (name, size, params, fps, use) in zip(model_cols, models):
            with col:
                st.markdown(f"""
                <div style="background:#f0f8ff; padding:10px; border-radius:5px; text-align:center;">
                    <h5>{name}</h5>
                    <p style="font-size:11px; margin:2px;">íŒŒë¼ë¯¸í„°: {params}</p>
                    <p style="font-size:11px; margin:2px;">{fps}</p>
                    <p style="font-size:10px; color:#666;">{use}</p>
                </div>
                """, unsafe_allow_html=True)

        st.markdown("---")

        with st.expander("ğŸ“¦ YOLOv8 ì„¤ì¹˜ ë° ì‚¬ìš©", expanded=True):
            st.code("""
# 1. ì„¤ì¹˜
pip install ultralytics

# 2. ê¸°ë³¸ ì‚¬ìš©
from ultralytics import YOLO
import cv2

# ëª¨ë¸ ë¡œë“œ
model = YOLO('yolov8m.pt')  # ë˜ëŠ” yolov8n.pt, yolov8s.pt ë“±

# ì´ë¯¸ì§€ ì¶”ë¡ 
results = model('road_video.mp4')  # ë¹„ë””ì˜¤/ì´ë¯¸ì§€/í´ë” ê²½ë¡œ

# ê²°ê³¼ ì ‘ê·¼
for result in results:
    boxes = result.boxes  # ë°”ìš´ë”© ë°•ìŠ¤
    for box in boxes:
        x1, y1, x2, y2 = box.xyxy[0]  # ì¢Œí‘œ
        conf = box.conf[0]             # ì‹ ë¢°ë„
        cls = box.cls[0]               # í´ë˜ìŠ¤
        label = model.names[int(cls)]   # í´ë˜ìŠ¤ ì´ë¦„

        print(f"{label}: {conf:.2f}")

# 3. ì‹¤ì‹œê°„ ì›¹ìº 
cap = cv2.VideoCapture(0)
while True:
    ret, frame = cap.read()
    results = model(frame)
    annotated = results[0].plot()  # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
    cv2.imshow('YOLOv8', annotated)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
            """, language='python')

        with st.expander("ğŸ¨ ì»¤ìŠ¤í…€ ì‹œê°í™”", expanded=True):
            st.code("""
def draw_custom_boxes(frame, results, conf_threshold=0.5):
    \"\"\"ì»¤ìŠ¤í…€ ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°\"\"\"

    # í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ ì •ì˜
    colors = {
        'car': (0, 255, 0),        # ì´ˆë¡
        'truck': (255, 255, 0),    # ë…¸ë‘
        'person': (0, 0, 255),     # ë¹¨ê°•
        'bicycle': (255, 0, 255),  # ìí™
        'motorcycle': (255, 128, 0) # ì£¼í™©
    }

    for result in results:
        boxes = result.boxes
        for box in boxes:
            # ì‹ ë¢°ë„ í•„í„°ë§
            conf = float(box.conf[0])
            if conf < conf_threshold:
                continue

            # ì¢Œí‘œ ë° í´ë˜ìŠ¤
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            cls = int(box.cls[0])
            label = result.names[cls]

            # ìƒ‰ìƒ ì„ íƒ
            color = colors.get(label, (255, 255, 255))

            # ë°”ìš´ë”© ë°•ìŠ¤ (ë‘ê»˜ 2)
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

            # ë¼ë²¨ ë°°ê²½
            label_text = f"{label} {conf:.2f}"
            (w, h), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)
            cv2.rectangle(frame, (x1, y1 - h - 10), (x1 + w, y1), color, -1)

            # ë¼ë²¨ í…ìŠ¤íŠ¸
            cv2.putText(frame, label_text, (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)

    return frame


# ì‚¬ìš© ì˜ˆì‹œ
cap = cv2.VideoCapture('road_video.mp4')
model = YOLO('yolov8m.pt')

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # YOLOv8 ì¶”ë¡ 
    results = model(frame, conf=0.3)  # ì‹ ë¢°ë„ 0.3 ì´ìƒë§Œ

    # ì»¤ìŠ¤í…€ ì‹œê°í™”
    frame = draw_custom_boxes(frame, results, conf_threshold=0.5)

    cv2.imshow('Custom Visualization', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
            """, language='python')

        with st.expander("âš¡ ì„±ëŠ¥ ìµœì í™”", expanded=True):
            st.markdown("""
            ### 1. ì…ë ¥ í¬ê¸° ì¡°ì •

            ```python
            # ì‘ì€ ì…ë ¥ â†’ ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ ë‚®ìŒ
            results = model(frame, imgsz=320)  # ê¸°ë³¸ 640

            # í° ì…ë ¥ â†’ ëŠë¦¬ì§€ë§Œ ì •í™•ë„ ë†’ìŒ
            results = model(frame, imgsz=1280)
            ```

            ### 2. ë°°ì¹˜ ì²˜ë¦¬

            ```python
            # ì—¬ëŸ¬ í”„ë ˆì„ ë™ì‹œ ì²˜ë¦¬
            frames = [frame1, frame2, frame3]
            results = model(frames, batch=3)
            ```

            ### 3. TensorRT ìµœì í™”

            ```python
            # ONNX ë³€í™˜
            model.export(format='onnx')

            # TensorRT ì—”ì§„ ë¹Œë“œ (NVIDIA GPU)
            model.export(format='engine')  # .engine íŒŒì¼ ìƒì„±

            # TensorRT ëª¨ë¸ ë¡œë“œ (10ë°° ë¹ ë¦„!)
            model = YOLO('yolov8m.engine')
            ```

            ### 4. í•˜í”„ ì •ë°€ë„ (FP16)

            ```python
            # GPU ë©”ëª¨ë¦¬ ì ˆì•½ + ì†ë„ í–¥ìƒ
            results = model(frame, half=True)
            ```

            ### 5. í”„ë ˆì„ ìŠ¤í‚µ

            ```python
            frame_skip = 2  # 2í”„ë ˆì„ë§ˆë‹¤ ì²˜ë¦¬
            frame_count = 0

            while cap.isOpened():
                ret, frame = cap.read()
                frame_count += 1

                if frame_count % frame_skip == 0:
                    results = model(frame)
            ```
            """)

    def _render_bytetrack(self):
        """ByteTrack ê°ì²´ ì¶”ì """
        st.subheader("ğŸ”— ByteTrack ê°ì²´ ì¶”ì ")

        st.markdown("""
        **ê°ì²´ ì¶”ì  (Object Tracking)**: í”„ë ˆì„ ê°„ ê°™ì€ ê°ì²´ì— ì¼ê´€ëœ ID ë¶€ì—¬

        **í•„ìš”ì„±**:
        - ì°¨ëŸ‰ í–‰ë™ ì˜ˆì¸¡ (ì†ë„, ë°©í–¥)
        - ì°¨ê°„ ê±°ë¦¬ ëª¨ë‹ˆí„°ë§
        - ìœ„í—˜ ì°¨ëŸ‰ ì‹ë³„

        **ByteTrack íŠ¹ì§•**:
        - SOTA ì„±ëŠ¥ (MOT20 ê¸°ì¤€ 80.3% MOTA)
        - ë‚®ì€ ì‹ ë¢°ë„ ê°ì²´ë„ í™œìš©
        - ë¹ ë¥¸ ì†ë„ (~30 FPS)
        """)

        st.markdown("---")

        # ByteTrack ì•Œê³ ë¦¬ì¦˜
        with st.expander("ğŸ§  ByteTrack ì•Œê³ ë¦¬ì¦˜", expanded=True):
            st.markdown("""
            ### ê¸°ì¡´ Trackingì˜ ë¬¸ì œì 

            **ì „í†µì  ë°©ì‹**: ë†’ì€ ì‹ ë¢°ë„ íƒì§€ë§Œ ì‚¬ìš©
            ```
            YOLOv8 íƒì§€ â†’ conf > 0.7 í•„í„°ë§ â†’ Tracking
            ```

            **ë¬¸ì œ**:
            - ê°€ë ¤ì§„ ê°ì²´ (ë‚®ì€ ì‹ ë¢°ë„) ë¬´ì‹œ
            - ID ìŠ¤ìœ„ì¹­ ë¹ˆë²ˆ ë°œìƒ
            - í”„ë ˆì„ ê°„ ë¶ˆì—°ì†

            ---

            ### ByteTrackì˜ í•´ê²°ì±…

            **2-Stage ë§¤ì¹­**:

            ```
            YOLOv8 íƒì§€
                â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ ë†’ì€ ì‹ ë¢°ë„  â”‚ ë‚®ì€ ì‹ ë¢°ë„  â”‚
            â”‚ (conf > 0.7)â”‚ (0.1 < confâ”‚
            â”‚             â”‚     < 0.7)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“              â†“
            [1ì°¨ ë§¤ì¹­]   [2ì°¨ ë§¤ì¹­]
            ê¸°ì¡´ íŠ¸ë™ê³¼   1ì°¨ì—ì„œ ì•ˆ ë§ì€
            ë§¤ì¹­          íŠ¸ë™ê³¼ ë§¤ì¹­
                â†“
            ìµœì¢… íŠ¸ë™ ì—…ë°ì´íŠ¸
            ```

            ---

            ### IoU (Intersection over Union) ë§¤ì¹­

            ```python
            def iou(box1, box2):
                \"\"\"ë‘ ë°•ìŠ¤ì˜ IoU ê³„ì‚°\"\"\"
                x1 = max(box1[0], box2[0])
                y1 = max(box1[1], box2[1])
                x2 = min(box1[2], box2[2])
                y2 = min(box1[3], box2[3])

                intersection = max(0, x2 - x1) * max(0, y2 - y1)
                area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
                area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
                union = area1 + area2 - intersection

                return intersection / union if union > 0 else 0


            # ë§¤ì¹­ ì˜ˆì‹œ
            detection = [100, 100, 200, 200]  # ìƒˆ íƒì§€
            track = [105, 105, 205, 205]      # ê¸°ì¡´ íŠ¸ë™

            iou_score = iou(detection, track)  # 0.81 (ë†’ìŒ â†’ ê°™ì€ ê°ì²´!)

            if iou_score > 0.5:
                track.update(detection)  # íŠ¸ë™ ì—…ë°ì´íŠ¸
            ```

            ---

            ### ì¹¼ë§Œ í•„í„° ì˜ˆì¸¡

            ```python
            class KalmanTracker:
                def __init__(self, bbox):
                    self.kf = KalmanFilter(dim_x=7, dim_z=4)
                    # ìƒíƒœ: [x, y, s, r, vx, vy, vs]
                    # ê´€ì¸¡: [x, y, s, r]
                    self.kf.x[:4] = bbox

                def predict(self):
                    \"\"\"ë‹¤ìŒ í”„ë ˆì„ ìœ„ì¹˜ ì˜ˆì¸¡\"\"\"
                    self.kf.predict()
                    return self.kf.x[:4]

                def update(self, bbox):
                    \"\"\"ê´€ì¸¡ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸\"\"\"
                    self.kf.update(bbox)


            # ì‚¬ìš© ì˜ˆì‹œ
            tracker = KalmanTracker([100, 100, 50, 50])

            # í”„ë ˆì„ t
            predicted = tracker.predict()  # [102, 105, 50, 50]

            # í”„ë ˆì„ t+1 (ì‹¤ì œ íƒì§€)
            detected = [103, 106, 51, 51]
            tracker.update(detected)
            ```
            """)

        with st.expander("ğŸ“¦ ByteTrack ì„¤ì¹˜ ë° ì‚¬ìš©", expanded=True):
            st.code("""
# 1. ì„¤ì¹˜
pip install bytetrack

# 2. ê¸°ë³¸ ì‚¬ìš©
from bytetrack import BYTETracker
from ultralytics import YOLO
import numpy as np

# YOLOv8 + ByteTrack ì´ˆê¸°í™”
model = YOLO('yolov8m.pt')
tracker = BYTETracker(
    track_thresh=0.5,      # íŠ¸ë™ ì‹ ë¢°ë„ ì„ê³„ê°’
    track_buffer=30,       # ìµœëŒ€ í”„ë ˆì„ ë²„í¼
    match_thresh=0.8,      # ë§¤ì¹­ IoU ì„ê³„ê°’
    frame_rate=30          # ë¹„ë””ì˜¤ FPS
)

cap = cv2.VideoCapture('road_video.mp4')

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # YOLOv8 íƒì§€
    results = model(frame)

    # ByteTrack í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    detections = []
    for result in results:
        boxes = result.boxes
        for box in boxes:
            x1, y1, x2, y2 = map(float, box.xyxy[0])
            conf = float(box.conf[0])
            cls = int(box.cls[0])

            # [x1, y1, x2, y2, score, class]
            detections.append([x1, y1, x2, y2, conf, cls])

    detections = np.array(detections)

    # ByteTrack ì—…ë°ì´íŠ¸
    if len(detections) > 0:
        online_targets = tracker.update(detections, [frame.shape[0], frame.shape[1]])

        # íŠ¸ë™ ê·¸ë¦¬ê¸°
        for track in online_targets:
            tlwh = track.tlwh  # [top, left, width, height]
            track_id = track.track_id

            x1, y1 = int(tlwh[0]), int(tlwh[1])
            x2, y2 = int(tlwh[0] + tlwh[2]), int(tlwh[1] + tlwh[3])

            # ë°”ìš´ë”© ë°•ìŠ¤ + ID
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, f"ID: {track_id}", (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    cv2.imshow('ByteTrack', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
            """, language='python')

        st.markdown("---")

        # íŠ¸ë™ ì •ë³´ í™œìš©
        with st.expander("ğŸ“Š íŠ¸ë™ ì •ë³´ í™œìš©", expanded=True):
            st.markdown("""
            ### 1. ì†ë„ ê³„ì‚°

            ```python
            class VelocityTracker:
                def __init__(self):
                    self.tracks = {}  # {track_id: [positions]}

                def update(self, track_id, bbox, fps=30):
                    if track_id not in self.tracks:
                        self.tracks[track_id] = []

                    # ì¤‘ì‹¬ì  ì €ì¥
                    cx = (bbox[0] + bbox[2]) / 2
                    cy = (bbox[1] + bbox[3]) / 2
                    self.tracks[track_id].append((cx, cy))

                    # ìµœê·¼ 10 í”„ë ˆì„ë§Œ ìœ ì§€
                    if len(self.tracks[track_id]) > 10:
                        self.tracks[track_id].pop(0)

                    # ì†ë„ ê³„ì‚° (í”½ì…€/ì´ˆ)
                    if len(self.tracks[track_id]) >= 2:
                        p1 = self.tracks[track_id][-2]
                        p2 = self.tracks[track_id][-1]

                        dx = p2[0] - p1[0]
                        dy = p2[1] - p1[1]
                        distance = np.sqrt(dx**2 + dy**2)

                        velocity = distance * fps  # í”½ì…€/ì´ˆ

                        # í”½ì…€ â†’ ë¯¸í„° ë³€í™˜ (ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í•„ìš”)
                        velocity_mps = velocity * 0.05  # ì˜ˆ: 0.05m/í”½ì…€
                        velocity_kmh = velocity_mps * 3.6

                        return velocity_kmh

                    return 0.0


            # ì‚¬ìš© ì˜ˆì‹œ
            vel_tracker = VelocityTracker()

            for track in online_targets:
                velocity = vel_tracker.update(track.track_id, track.tlbr)
                print(f"ID {track.track_id}: {velocity:.1f} km/h")
            ```

            ---

            ### 2. ê¶¤ì  ì˜ˆì¸¡

            ```python
            def predict_trajectory(tracks, num_future_frames=5):
                \"\"\"ì„ í˜• ë³´ê°„ìœ¼ë¡œ ë¯¸ë˜ ìœ„ì¹˜ ì˜ˆì¸¡\"\"\"
                if len(tracks) < 3:
                    return None

                # ìµœê·¼ 3ê°œ ìœ„ì¹˜
                recent = tracks[-3:]
                x_coords = [p[0] for p in recent]
                y_coords = [p[1] for p in recent]

                # ì„ í˜• íšŒê·€
                t = np.arange(len(recent))
                vx = np.polyfit(t, x_coords, 1)[0]
                vy = np.polyfit(t, y_coords, 1)[0]

                # ë¯¸ë˜ ìœ„ì¹˜ ì˜ˆì¸¡
                future_positions = []
                for i in range(1, num_future_frames + 1):
                    future_x = x_coords[-1] + vx * i
                    future_y = y_coords[-1] + vy * i
                    future_positions.append((future_x, future_y))

                return future_positions


            # ì‹œê°í™”
            for track_id, positions in vel_tracker.tracks.items():
                future = predict_trajectory(positions)
                if future:
                    for i, (fx, fy) in enumerate(future):
                        cv2.circle(frame, (int(fx), int(fy)),
                                   3, (255, 0, 0), -1)
            ```

            ---

            ### 3. ì¶©ëŒ ìœ„í—˜ ê°ì§€

            ```python
            def check_collision_risk(ego_track, other_track, threshold=50):
                \"\"\"ë‘ ì°¨ëŸ‰ì˜ ì¶©ëŒ ìœ„í—˜ ê³„ì‚°\"\"\"

                # í˜„ì¬ ê±°ë¦¬
                ego_center = ((ego_track[0] + ego_track[2]) / 2,
                              (ego_track[1] + ego_track[3]) / 2)
                other_center = ((other_track[0] + other_track[2]) / 2,
                                (other_track[1] + other_track[3]) / 2)

                distance = np.sqrt(
                    (ego_center[0] - other_center[0])**2 +
                    (ego_center[1] - other_center[1])**2
                )

                # TTC (Time To Collision)
                ego_vel = vel_tracker.update(ego_id, ego_track)
                other_vel = vel_tracker.update(other_id, other_track)

                relative_vel = abs(ego_vel - other_vel)

                if relative_vel > 0:
                    ttc = distance / relative_vel
                else:
                    ttc = float('inf')

                # ìœ„í—˜ íŒì •
                if ttc < 2.0:  # 2ì´ˆ ì´ë‚´
                    return "HIGH", ttc
                elif ttc < 5.0:
                    return "MEDIUM", ttc
                else:
                    return "LOW", ttc
            ```
            """)

    def _render_ipm(self):
        """IPM ê±°ë¦¬ ì¶”ì •"""
        st.subheader("ğŸ“ IPM (Inverse Perspective Mapping) ê±°ë¦¬ ì¶”ì •")

        st.markdown("""
        **IPM**: ì¹´ë©”ë¼ ì˜ìƒì„ Bird's Eye View(BEV)ë¡œ ë³€í™˜í•˜ì—¬ ì‹¤ì œ ê±°ë¦¬ ê³„ì‚°

        **ëª©ì **:
        - í”½ì…€ ì¢Œí‘œ â†’ ì‹¤ì œ ë¯¸í„° ë‹¨ìœ„ ë³€í™˜
        - ì°¨ê°„ ê±°ë¦¬ ì •í™•íˆ ê³„ì‚°
        - ì¶©ëŒ ìœ„í—˜ ì •ëŸ‰í™”
        """)

        st.markdown("---")

        with st.expander("ğŸ¯ IPM ì›ë¦¬", expanded=True):
            st.markdown("""
            ### ì¹´ë©”ë¼ ëª¨ë¸

            **í•€í™€ ì¹´ë©”ë¼ ëª¨ë¸**:

            ```
            3D ì›”ë“œ ì¢Œí‘œ (X, Y, Z)
                â†“
            [ì¹´ë©”ë¼ ì™¸ë¶€ íŒŒë¼ë¯¸í„°]
            íšŒì „(R), ì´ë™(t)
                â†“
            ì¹´ë©”ë¼ ì¢Œí‘œê³„ (Xc, Yc, Zc)
                â†“
            [ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°]
            ì´ˆì ê±°ë¦¬(f), ì£¼ì (cx, cy)
                â†“
            2D ì´ë¯¸ì§€ ì¢Œí‘œ (u, v)
            ```

            **íˆ¬ì˜ ë°©ì •ì‹**:
            ```
            u = fx * (Xc / Zc) + cx
            v = fy * (Yc / Zc) + cy
            ```

            ---

            ### ì—­ë³€í™˜ (IPM)

            **ê°€ì •**: Z=0 (í‰ë©´ ë„ë¡œ)

            ```python
            def pixel_to_world(u, v, camera_matrix, extrinsics):
                \"\"\"í”½ì…€ ì¢Œí‘œ â†’ ì›”ë“œ ì¢Œí‘œ (Z=0)\"\"\"

                # ë‚´ë¶€ íŒŒë¼ë¯¸í„°
                fx, fy = camera_matrix[0, 0], camera_matrix[1, 1]
                cx, cy = camera_matrix[0, 2], camera_matrix[1, 2]

                # ì •ê·œí™” ì¢Œí‘œ
                xn = (u - cx) / fx
                yn = (v - cy) / fy

                # ì™¸ë¶€ íŒŒë¼ë¯¸í„° (íšŒì „ + ì´ë™)
                R = extrinsics[:3, :3]
                t = extrinsics[:3, 3]

                # ì—­ë³€í™˜
                # [X, Y, Z]^T = R^-1 * (s * [xn, yn, 1]^T - t)
                # Z=0 ì¡°ê±´ ì‚¬ìš©

                # ... (ë³µì¡í•œ í–‰ë ¬ ê³„ì‚°) ...

                return X, Y  # ë¯¸í„° ë‹¨ìœ„
            ```

            ---

            ### ê°„ë‹¨í•œ ê·¼ì‚¬ ë°©ë²•

            ```python
            def simple_distance_estimation(bbox_bottom_y, camera_height=1.5):
                \"\"\"ë°•ìŠ¤ í•˜ë‹¨ yì¢Œí‘œë¡œ ê±°ë¦¬ ê·¼ì‚¬\"\"\"

                # ì¹´ë©”ë¼ ë†’ì´: 1.5m (ì§€ë©´ì—ì„œ)
                # ì´ë¯¸ì§€ ë†’ì´: 720 í”½ì…€
                # ìˆ˜í‰ì„  (vanishing point): y=360

                horizon_y = 360
                image_height = 720

                # yì¢Œí‘œê°€ ìˆ˜í‰ì„ ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë©€ë¦¬ ìˆìŒ
                if bbox_bottom_y <= horizon_y:
                    return float('inf')  # í•˜ëŠ˜/ë°°ê²½

                # ê°„ë‹¨í•œ ë¹„ë¡€ì‹
                # distance âˆ 1 / (bbox_bottom_y - horizon_y)

                distance = (image_height - horizon_y) / (bbox_bottom_y - horizon_y)
                distance *= 50  # ìŠ¤ì¼€ì¼ ì¡°ì • (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í•„ìš”)

                return distance  # ë¯¸í„°


            # ì‚¬ìš© ì˜ˆì‹œ
            for track in online_targets:
                tlwh = track.tlwh
                bottom_y = tlwh[1] + tlwh[3]

                distance = simple_distance_estimation(bottom_y)

                print(f"ID {track.track_id}: {distance:.1f}m")
            ```
            """)

        with st.expander("ğŸ¥ ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜", expanded=True):
            st.markdown("""
            ### OpenCV ì²´ìŠ¤ë³´ë“œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜

            ```python
            import cv2
            import numpy as np
            import glob

            # 1. ì²´ìŠ¤ë³´ë“œ ì´ë¯¸ì§€ ì´¬ì˜ (20-30ì¥)
            # ì²´ìŠ¤ë³´ë“œ í¬ê¸°: 9x6 (ë‚´ë¶€ ì½”ë„ˆ ìˆ˜)

            # 2. ì½”ë„ˆ ì  ì°¾ê¸°
            objp = np.zeros((6*9, 3), np.float32)
            objp[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)

            objpoints = []  # 3D ì 
            imgpoints = []  # 2D ì 

            images = glob.glob('calibration/*.jpg')

            for fname in images:
                img = cv2.imread(fname)
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

                # ì½”ë„ˆ ì°¾ê¸°
                ret, corners = cv2.findChessboardCorners(gray, (9, 6), None)

                if ret:
                    objpoints.append(objp)
                    imgpoints.append(corners)

            # 3. ìº˜ë¦¬ë¸Œë ˆì´ì…˜
            ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(
                objpoints, imgpoints, gray.shape[::-1], None, None
            )

            print("Camera Matrix:")
            print(camera_matrix)
            # [[fx,  0, cx],
            #  [ 0, fy, cy],
            #  [ 0,  0,  1]]

            print("\\nDistortion Coefficients:")
            print(dist_coeffs)
            # [k1, k2, p1, p2, k3]

            # 4. ì™œê³¡ ë³´ì •
            img = cv2.imread('test.jpg')
            h, w = img.shape[:2]
            new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(
                camera_matrix, dist_coeffs, (w, h), 1, (w, h)
            )

            undistorted = cv2.undistort(img, camera_matrix, dist_coeffs,
                                        None, new_camera_matrix)

            cv2.imshow('Undistorted', undistorted)
            ```

            ---

            ### ìˆ˜ë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ê°„ë‹¨)

            ```python
            # ì‹¤ì œ ì¸¡ì •ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì„¤ì •

            # 1. ì¹´ë©”ë¼ ë†’ì´ ì¸¡ì •
            camera_height = 1.5  # ë¯¸í„°

            # 2. ì•Œë ¤ì§„ ê±°ë¦¬ì— ë¬¼ì²´ ë°°ì¹˜ (ì˜ˆ: 10m, 20m)
            # 3. í”½ì…€ yì¢Œí‘œ ê¸°ë¡
            # 4. ë£©ì—… í…Œì´ë¸” ìƒì„±

            distance_lut = {
                680: 5,    # y=680 í”½ì…€ â†’ 5ë¯¸í„°
                650: 10,   # y=650 í”½ì…€ â†’ 10ë¯¸í„°
                620: 15,
                590: 20,
                560: 25,
                530: 30
            }

            def lookup_distance(y):
                \"\"\"LUTë¡œ ê±°ë¦¬ ì¶”ì •\"\"\"
                # ì„ í˜• ë³´ê°„
                keys = sorted(distance_lut.keys(), reverse=True)
                for i in range(len(keys)-1):
                    if keys[i] >= y >= keys[i+1]:
                        y1, d1 = keys[i], distance_lut[keys[i]]
                        y2, d2 = keys[i+1], distance_lut[keys[i+1]]
                        # ì„ í˜• ë³´ê°„
                        distance = d1 + (y - y1) * (d2 - d1) / (y2 - y1)
                        return distance
                return distance_lut[keys[-1]]
            ```
            """)

        st.markdown("---")

        # ì „ì²´ ì˜ˆì‹œ
        with st.expander("ğŸ“‹ ê±°ë¦¬ ì¶”ì • ì „ì²´ ì½”ë“œ", expanded=False):
            st.code("""
import cv2
import numpy as np
from ultralytics import YOLO
from bytetrack import BYTETracker

# ëª¨ë¸ ì´ˆê¸°í™”
model = YOLO('yolov8m.pt')
tracker = BYTETracker()

# ê°„ë‹¨í•œ ê±°ë¦¬ ì¶”ì • í•¨ìˆ˜
def estimate_distance(bbox_bottom_y, image_height=720):
    horizon_y = image_height * 0.5  # ìˆ˜í‰ì„ : ì¤‘ê°„

    if bbox_bottom_y <= horizon_y:
        return float('inf')

    distance = (image_height - horizon_y) / (bbox_bottom_y - horizon_y)
    distance *= 50  # ìŠ¤ì¼€ì¼ (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í•„ìš”)

    return min(distance, 100)  # ìµœëŒ€ 100m


# ë¹„ë””ì˜¤ ì²˜ë¦¬
cap = cv2.VideoCapture('road_video.mp4')

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    height, width = frame.shape[:2]

    # 1. YOLOv8 íƒì§€
    results = model(frame)

    # 2. ByteTrack ë³€í™˜
    detections = []
    for result in results:
        for box in result.boxes:
            x1, y1, x2, y2 = map(float, box.xyxy[0])
            conf = float(box.conf[0])
            cls = int(box.cls[0])
            detections.append([x1, y1, x2, y2, conf, cls])

    detections = np.array(detections)

    # 3. ByteTrack ì¶”ì 
    if len(detections) > 0:
        online_targets = tracker.update(detections, [height, width])

        for track in online_targets:
            tlwh = track.tlwh
            x1, y1 = int(tlwh[0]), int(tlwh[1])
            x2, y2 = int(tlwh[0] + tlwh[2]), int(tlwh[1] + tlwh[3])
            track_id = track.track_id

            # 4. ê±°ë¦¬ ì¶”ì •
            bottom_y = y2
            distance = estimate_distance(bottom_y, height)

            # 5. ì‹œê°í™”
            color = (0, 255, 0) if distance > 20 else (0, 165, 255) if distance > 10 else (0, 0, 255)

            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

            label = f"ID:{track_id} {distance:.1f}m"
            cv2.putText(frame, label, (x1, y1-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    cv2.imshow('Distance Estimation', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
            """, language='python')

    def _render_object_full_code(self):
        """ê°ì²´ íƒì§€/ì¶”ì  ì „ì²´ ì½”ë“œ"""
        st.subheader("ğŸ“‹ ì „ì²´ í†µí•© ì½”ë“œ")

        st.markdown("""
        **í†µí•© ê¸°ëŠ¥**:
        - YOLOv8 íƒì§€
        - ByteTrack ì¶”ì 
        - ê±°ë¦¬ ì¶”ì •
        - ì†ë„ ê³„ì‚°
        - ì¶©ëŒ ìœ„í—˜ ë¶„ì„
        """)

        st.code("""
# Week 10: ê°ì²´ íƒì§€ ë° ì¶”ì  (ì™„ì „íŒ)

import cv2
import numpy as np
from ultralytics import YOLO
from bytetrack import BYTETracker
from collections import defaultdict


class AutonomousDrivingPerception:
    \"\"\"ììœ¨ì£¼í–‰ ì¸ì‹ ì‹œìŠ¤í…œ\"\"\"

    def __init__(self, model_path='yolov8m.pt'):
        # YOLOv8 ëª¨ë¸
        self.model = YOLO(model_path)

        # ByteTrack
        self.tracker = BYTETracker(
            track_thresh=0.5,
            track_buffer=30,
            match_thresh=0.8,
            frame_rate=30
        )

        # íŠ¸ë™ íˆìŠ¤í† ë¦¬
        self.track_history = defaultdict(list)

        # í”„ë ˆì„ ì¹´ìš´í„°
        self.frame_count = 0

    def estimate_distance(self, bbox_bottom_y, image_height):
        \"\"\"ê°„ë‹¨í•œ ê±°ë¦¬ ì¶”ì •\"\"\"
        horizon_y = image_height * 0.5

        if bbox_bottom_y <= horizon_y:
            return float('inf')

        distance = (image_height - horizon_y) / (bbox_bottom_y - horizon_y)
        distance *= 50

        return min(distance, 100)

    def calculate_velocity(self, track_id, current_pos, fps=30):
        \"\"\"ì†ë„ ê³„ì‚° (km/h)\"\"\"
        self.track_history[track_id].append(current_pos)

        # ìµœê·¼ 10í”„ë ˆì„ë§Œ ìœ ì§€
        if len(self.track_history[track_id]) > 10:
            self.track_history[track_id].pop(0)

        if len(self.track_history[track_id]) >= 2:
            p1 = self.track_history[track_id][-2]
            p2 = self.track_history[track_id][-1]

            dx = p2[0] - p1[0]
            dy = p2[1] - p1[1]
            distance_px = np.sqrt(dx**2 + dy**2)

            velocity_pxps = distance_px * fps
            velocity_mps = velocity_pxps * 0.05  # í”½ì…€â†’ë¯¸í„° (ìº˜ë¦¬ë¸Œë ˆì´ì…˜)
            velocity_kmh = velocity_mps * 3.6

            return velocity_kmh

        return 0.0

    def assess_risk(self, distance, velocity):
        \"\"\"ìœ„í—˜ë„ í‰ê°€\"\"\"
        # TTC (Time To Collision)
        if velocity > 0:
            ttc = distance / (velocity / 3.6)  # km/h â†’ m/s
        else:
            ttc = float('inf')

        if ttc < 2.0:
            return "HIGH", ttc
        elif ttc < 5.0:
            return "MEDIUM", ttc
        else:
            return "LOW", ttc

    def process_frame(self, frame):
        \"\"\"í”„ë ˆì„ ì²˜ë¦¬\"\"\"
        self.frame_count += 1
        height, width = frame.shape[:2]

        # 1. YOLOv8 íƒì§€
        results = self.model(frame, verbose=False)

        # 2. ByteTrack ë³€í™˜
        detections = []
        for result in results:
            for box in result.boxes:
                x1, y1, x2, y2 = map(float, box.xyxy[0])
                conf = float(box.conf[0])
                cls = int(box.cls[0])
                detections.append([x1, y1, x2, y2, conf, cls])

        detections = np.array(detections)

        # 3. ByteTrack ì¶”ì 
        if len(detections) > 0:
            online_targets = self.tracker.update(detections, [height, width])

            for track in online_targets:
                tlwh = track.tlwh
                x1, y1 = int(tlwh[0]), int(tlwh[1])
                x2, y2 = int(tlwh[0] + tlwh[2]), int(tlwh[1] + tlwh[3])
                track_id = track.track_id

                # ì¤‘ì‹¬ì 
                cx = (x1 + x2) // 2
                cy = (y1 + y2) // 2

                # 4. ê±°ë¦¬ ì¶”ì •
                distance = self.estimate_distance(y2, height)

                # 5. ì†ë„ ê³„ì‚°
                velocity = self.calculate_velocity(track_id, (cx, cy))

                # 6. ìœ„í—˜ë„ í‰ê°€
                risk_level, ttc = self.assess_risk(distance, velocity)

                # 7. ì‹œê°í™”
                # ìœ„í—˜ë„ë³„ ìƒ‰ìƒ
                colors = {
                    "HIGH": (0, 0, 255),      # ë¹¨ê°•
                    "MEDIUM": (0, 165, 255),  # ì£¼í™©
                    "LOW": (0, 255, 0)        # ì´ˆë¡
                }
                color = colors.get(risk_level, (255, 255, 255))

                # ë°”ìš´ë”© ë°•ìŠ¤
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

                # ì •ë³´ í…ìŠ¤íŠ¸
                label = f"ID:{track_id} {distance:.1f}m {velocity:.0f}km/h"
                cv2.putText(frame, label, (x1, y1-10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

                # ìœ„í—˜ë„ í‘œì‹œ
                if risk_level != "LOW":
                    risk_text = f"{risk_level} (TTC:{ttc:.1f}s)"
                    cv2.putText(frame, risk_text, (x1, y1-30),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

                # ê¶¤ì  ê·¸ë¦¬ê¸°
                if len(self.track_history[track_id]) > 1:
                    points = np.array(self.track_history[track_id], dtype=np.int32)
                    cv2.polylines(frame, [points], False, color, 2)

        # FPS í‘œì‹œ
        fps_text = f"Frame: {self.frame_count}"
        cv2.putText(frame, fps_text, (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

        return frame


def main():
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    perception = AutonomousDrivingPerception('yolov8m.pt')

    # ë¹„ë””ì˜¤ ë¡œë“œ
    cap = cv2.VideoCapture('road_video.mp4')

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # í”„ë ˆì„ ì²˜ë¦¬
        result = perception.process_frame(frame)

        # ê²°ê³¼ í‘œì‹œ
        cv2.imshow('Autonomous Driving Perception', result)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
        """, language='python')

        st.markdown("---")

        st.success("""
        âœ… **ì™„ì„±ëœ ê¸°ëŠ¥**:
        - YOLOv8 ì‹¤ì‹œê°„ ê°ì²´ íƒì§€
        - ByteTrack ID ìœ ì§€ ì¶”ì 
        - ê±°ë¦¬ ì¶”ì • (IPM ê·¼ì‚¬)
        - ì†ë„ ê³„ì‚° (km/h)
        - TTC ê¸°ë°˜ ì¶©ëŒ ìœ„í—˜ ë¶„ì„
        - ìƒ‰ìƒë³„ ìœ„í—˜ë„ í‘œì‹œ
        - ê¶¤ì  ì‹œê°í™”

        ë‹¤ìŒ íƒ­ì—ì„œ ì´ ëª¨ë“ˆë“¤ì„ í†µí•©í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤!
        """)

    # ==================== Tab 4-7ì€ ë‹¤ìŒ ë©”ì‹œì§€ì—ì„œ ê³„ì† ====================

    def render_integrated_pipeline(self):
        """í†µí•© íŒŒì´í”„ë¼ì¸ (ê°„ë‹¨ ë²„ì „)"""
        st.header("ğŸ”— í†µí•© íŒŒì´í”„ë¼ì¸")
        st.info("ì°¨ì„  ì¸ì‹ + ê°ì²´ íƒì§€/ì¶”ì ì„ í†µí•©í•œ ì™„ì „í•œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì „ì²´ ì½”ë“œëŠ” Colab ë…¸íŠ¸ë¶ì„ ì°¸ì¡°í•˜ì„¸ìš”.")

    def render_3d_visualization(self):
        """3D ì‹œê°í™” (ê°„ë‹¨ ë²„ì „)"""
        st.header("ğŸ“ 3D ì‹œê°í™” (BEV)")
        st.info("Bird's Eye View ë³€í™˜ ë° 3D ë°”ìš´ë”© ë°•ìŠ¤ ì‹œê°í™”ì…ë‹ˆë‹¤. ì „ì²´ êµ¬í˜„ì€ Colab ë…¸íŠ¸ë¶ì„ ì°¸ì¡°í•˜ì„¸ìš”.")

    def render_simulator(self):
        """ê³ ê¸‰ ì‹œë®¬ë ˆì´í„° (ê°„ë‹¨ ë²„ì „)"""
        st.header("ğŸ® ê³ ê¸‰ ì‹œë®¬ë ˆì´í„°")
        st.info("êµì°¨ë¡œ, ì‹ í˜¸ë“±, ë‚ ì”¨ íš¨ê³¼ë¥¼ í¬í•¨í•œ ì‹œë®¬ë ˆì´í„°ì…ë‹ˆë‹¤. ì „ì²´ êµ¬í˜„ì€ Colab ë…¸íŠ¸ë¶ì„ ì°¸ì¡°í•˜ì„¸ìš”.")

    def render_deployment(self):
        """ì‹¤ì „ ë°°í¬ (ê°„ë‹¨ ë²„ì „)"""
        st.header("ğŸ’» ì‹¤ì „ ë°°í¬")
        st.info("TensorRT ìµœì í™” ë° Edge ë””ë°”ì´ìŠ¤ ë°°í¬ ê°€ì´ë“œì…ë‹ˆë‹¤. ì „ì²´ êµ¬í˜„ì€ Colab ë…¸íŠ¸ë¶ì„ ì°¸ì¡°í•˜ì„¸ìš”.")
