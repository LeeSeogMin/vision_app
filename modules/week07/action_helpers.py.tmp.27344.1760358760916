"""
행동인식 헬퍼 클래스
3-tier fallback 전략: HuggingFace Transformers → OpenCV → Simulation
"""

import numpy as np
from PIL import Image
import streamlit as st
from typing import Optional, List, Dict, Tuple, Any, Union
import warnings
import sys
import os

# BaseImageProcessor import
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from core.base_processor import BaseImageProcessor


class VideoHelper(BaseImageProcessor):
    """
    비디오 행동인식 헬퍼 클래스
    - 1순위: HuggingFace Transformers (transformers 패키지 + VideoMAE/TimeSformer 모델)
    - 2순위: OpenCV only (비디오 처리, Optical Flow만 가능, ML 모델 없음)
    - 3순위: Simulation mode (기본 비디오 처리 시뮬레이션)
    """

    def __init__(self):
        """
        VideoHelper 초기화
        - 3-tier fallback으로 사용 가능한 모드 감지
        - 디바이스 감지 (CPU/GPU)
        """
        super().__init__()
        self.mode = None  # 'transformers', 'opencv', 'simulation'
        self.device = None  # 'cuda', 'cpu', None
        self.model = None
        self.processor = None
        self.pipeline = None

        self._initialize()

    def _initialize(self):
        """
        3-tier fallback으로 초기화
        """
        # Tier 1: Try HuggingFace Transformers
        if self._try_transformers():
            self.mode = 'transformers'
            st.success("✅ 행동인식 준비 완료 (HuggingFace Transformers)")
            return

        # Tier 2: Try OpenCV only
        if self._try_opencv():
            self.mode = 'opencv'
            st.info("ℹ️ OpenCV 모드 활성화 (비디오 처리 가능, ML 모델 미사용)\n\n"
                   "행동 분류 기능을 사용하려면:\n"
                   "```bash\n"
                   "pip install transformers torch\n"
                   "```")
            return

        # Tier 3: Fallback to simulation
        self.mode = 'simulation'
        st.warning("⚠️ 시뮬레이션 모드 (실제 비디오 처리 미사용)\n\n"
                  "실제 기능을 사용하려면:\n"
                  "```bash\n"
                  "pip install opencv-python transformers torch\n"
                  "```")

    def _try_transformers(self) -> bool:
        """
        HuggingFace Transformers로 로드 시도
        VideoMAE, TimeSformer, X-CLIP 등 비디오 모델 지원
        """
        try:
            import transformers
            import torch

            # 디바이스 감지
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            if self.device == "cpu":
                st.info("ℹ️ GPU를 사용할 수 없어 CPU로 실행합니다. (비디오 처리는 느릴 수 있음)")

            # 필요한 모듈이 있는지만 확인 (실제 모델은 나중에 로드)
            return True

        except ImportError:
            return False
        except Exception as e:
            st.error(f"Transformers 초기화 실패: {e}")
            return False

    def _try_opencv(self) -> bool:
        """
        OpenCV 사용 가능 여부 확인
        비디오 처리와 Optical Flow는 가능하지만 ML 모델은 없음
        """
        try:
            import cv2
            return True
        except ImportError:
            return False
        except Exception as e:
            return False

    def _detect_device(self) -> Optional[str]:
        """
        CUDA 디바이스 감지
        Returns:
            'cuda', 'cpu', or None
        """
        try:
            import torch
            return "cuda" if torch.cuda.is_available() else "cpu"
        except ImportError:
            return None

    def get_mode(self) -> str:
        """
        현재 동작 모드 반환
        Returns:
            'transformers', 'opencv', 'simulation'
        """
        return self.mode

    def get_device(self) -> Optional[str]:
        """
        현재 디바이스 반환
        Returns:
            'cuda', 'cpu', or None
        """
        return self.device

    def is_available(self, feature: str) -> bool:
        """
        특정 기능 사용 가능 여부 확인

        Args:
            feature: 'video_processing', 'optical_flow', 'action_classification'

        Returns:
            bool: 기능 사용 가능 여부
        """
        if feature == 'video_processing':
            return self.mode in ['transformers', 'opencv']
        elif feature == 'optical_flow':
            return self.mode in ['transformers', 'opencv']
        elif feature == 'action_classification':
            return self.mode == 'transformers'
        else:
            return False

    def extract_frames(
        self,
        video_path: str,
        sample_rate: int = 30,
        max_frames: int = 100,
        target_size: Tuple[int, int] = (224, 224)
    ) -> List[np.ndarray]:
        """
        비디오에서 프레임 추출 (메모리 효율적)

        Args:
            video_path: 비디오 파일 경로
            sample_rate: 샘플링 레이트 (예: 30이면 30프레임당 1개 추출)
            max_frames: 최대 프레임 수 (메모리 제한)
            target_size: 출력 이미지 크기 (width, height)

        Returns:
            List[np.ndarray]: 프레임 리스트, 각 프레임 shape (H, W, 3) RGB
        """
        # Simulation mode: 랜덤 프레임 생성
        if self.mode == 'simulation':
            st.info("ℹ️ 시뮬레이션 모드: 랜덤 프레임 생성")
            frames = []
            for _ in range(min(10, max_frames)):
                frame = np.random.randint(0, 255, (target_size[1], target_size[0], 3), dtype=np.uint8)
                frames.append(frame)
            return frames

        # OpenCV or Transformers mode
        try:
            import cv2

            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"비디오 파일을 열 수 없습니다: {video_path}")

            # 비디오 정보 가져오기
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = int(cap.get(cv2.CAP_PROP_FPS))

            # 실제 샘플링 레이트 계산 (max_frames 제한 고려)
            actual_sample_rate = max(sample_rate, total_frames // max_frames) if total_frames > max_frames else sample_rate

            st.info(f"📹 비디오 정보: {total_frames}프레임, {fps}fps\n"
                   f"샘플링: 매 {actual_sample_rate}프레임당 1개 추출")

            frames = []
            frame_idx = 0

            # 프레임 추출
            while len(frames) < max_frames:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()

                if not ret:
                    break

                # BGR → RGB 변환
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # 리사이즈
                frame_resized = cv2.resize(frame_rgb, target_size, interpolation=cv2.INTER_LINEAR)

                frames.append(frame_resized)
                frame_idx += actual_sample_rate

            cap.release()

            st.success(f"✅ {len(frames)}개 프레임 추출 완료")
            return frames

        except ImportError:
            st.error("❌ OpenCV가 설치되지 않았습니다.")
            return []
        except Exception as e:
            st.error(f"❌ 프레임 추출 실패: {e}")
            return []

    def compute_optical_flow(
        self,
        frame1: np.ndarray,
        frame2: np.ndarray
    ) -> np.ndarray:
        """
        두 프레임 간 Optical Flow 계산 (Farneback 알고리즘)

        Args:
            frame1: 첫 번째 프레임 (H, W, 3) RGB
            frame2: 두 번째 프레임 (H, W, 3) RGB

        Returns:
            np.ndarray: Optical flow (H, W, 2) - [dx, dy] 모션 벡터
        """
        # Simulation mode: 랜덤 flow 생성
        if self.mode == 'simulation':
            h, w = frame1.shape[:2]
            flow = np.random.randn(h, w, 2).astype(np.float32) * 2
            return flow

        # OpenCV or Transformers mode
        try:
            import cv2

            # RGB → Grayscale
            gray1 = cv2.cvtColor(frame1, cv2.COLOR_RGB2GRAY)
            gray2 = cv2.cvtColor(frame2, cv2.COLOR_RGB2GRAY)

            # Farneback Optical Flow 계산
            flow = cv2.calcOpticalFlowFarneback(
                gray1, gray2,
                None,
                pyr_scale=0.5,    # 피라미드 스케일
                levels=3,          # 피라미드 레벨
                winsize=15,        # 윈도우 크기
                iterations=3,      # 반복 횟수
                poly_n=5,          # 다항식 확장
                poly_sigma=1.2,    # 가우시안 시그마
                flags=0
            )

            return flow

        except ImportError:
            st.error("❌ OpenCV가 설치되지 않았습니다.")
            return np.zeros((frame1.shape[0], frame1.shape[1], 2), dtype=np.float32)
        except Exception as e:
            st.error(f"❌ Optical Flow 계산 실패: {e}")
            return np.zeros((frame1.shape[0], frame1.shape[1], 2), dtype=np.float32)

    def visualize_flow(
        self,
        flow: np.ndarray
    ) -> np.ndarray:
        """
        Optical Flow를 HSV 색상 공간으로 시각화

        Args:
            flow: Optical flow (H, W, 2) - [dx, dy] 모션 벡터

        Returns:
            np.ndarray: RGB 이미지 (H, W, 3)
        """
        try:
            import cv2

            h, w = flow.shape[:2]

            # HSV 이미지 생성
            hsv = np.zeros((h, w, 3), dtype=np.uint8)
            hsv[..., 1] = 255  # Saturation을 최대로

            # Magnitude (크기)와 Angle (각도) 계산
            magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])

            # Hue: 방향 (0-180 범위)
            hsv[..., 0] = angle * 180 / np.pi / 2

            # Value: 크기 (정규화)
            hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)

            # HSV → RGB 변환
            rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)

            return rgb

        except ImportError:
            st.error("❌ OpenCV가 설치되지 않았습니다.")
            return np.zeros((flow.shape[0], flow.shape[1], 3), dtype=np.uint8)
        except Exception as e:
            st.error(f"❌ Flow 시각화 실패: {e}")
            return np.zeros((flow.shape[0], flow.shape[1], 3), dtype=np.uint8)


@st.cache_resource
def get_video_helper() -> VideoHelper:
    """
    캐시된 VideoHelper 인스턴스 반환 (싱글톤 패턴)

    Returns:
        VideoHelper: 캐시된 헬퍼 인스턴스
    """
    return VideoHelper()
