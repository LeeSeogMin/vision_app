"""
Week 3: Transfer Learning & Multi-modal API 모듈
Transfer Learning과 Multi-modal API 관련 기능을 제공합니다.
"""

import streamlit as st
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from typing import Dict, List, Tuple, Optional
import cv2
import matplotlib.pyplot as plt
import os
import sys

# 프로젝트 경로 추가
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from core.base_processor import BaseImageProcessor
from core.ai_models import AIModelManager
from .transfer_helpers import TransferLearningHelper
from .multimodal_helpers import MultiModalHelper


class TransferLearningModule(BaseImageProcessor):
    """Transfer Learning 및 Multi-modal API 학습 모듈"""

    def __init__(self):
        super().__init__()
        self.ai_manager = AIModelManager()
        self.transfer_helper = TransferLearningHelper()
        self.multimodal_helper = MultiModalHelper()

    def render(self):
        """Week 3 모듈 UI 렌더링 - Week 2와 동일한 메서드명"""
        self.render_ui()

    def render_ui(self):
        """Week 3 모듈 UI 렌더링"""
        st.title("🔄 Week 3: Transfer Learning & Multi-modal API")
        st.markdown("---")

        # 탭 생성
        tabs = st.tabs([
            "📚 이론",
            "🔄 Transfer Learning",
            "🖼️ CLIP 검색",
            "🎨 특징 추출",
            "📊 통합 분석",
            "🚀 실전 프로젝트",
            "🔍 API 비교"
        ])

        with tabs[0]:
            self._render_theory_tab()

        with tabs[1]:
            self._render_transfer_learning_tab()

        with tabs[2]:
            self._render_clip_search_tab()

        with tabs[3]:
            self._render_feature_extraction_tab()

        with tabs[4]:
            self._render_integrated_analysis_tab()

        with tabs[5]:
            self._render_project_tab()

        with tabs[6]:
            self._render_api_comparison_tab()

    def _render_theory_tab(self):
        """이론 탭"""
        st.header("📖 Transfer Learning & Multi-modal 이론")

        # 서브 탭 생성
        theory_tabs = st.tabs([
            "📚 Transfer Learning 이론",
            "🤖 CLIP 이론",
            "🔬 수학적 기초",
            "💡 실전 가이드"
        ])

        with theory_tabs[0]:
            self._render_transfer_learning_theory()

        with theory_tabs[1]:
            self._render_clip_theory()

        with theory_tabs[2]:
            self._render_mathematical_foundation()

        with theory_tabs[3]:
            self._render_practical_guide()

    def _render_transfer_learning_theory(self):
        """Transfer Learning 상세 이론"""
        st.markdown("## 📚 Transfer Learning 이론과 실습")

        # 개념과 배경
        with st.expander("### 1. Transfer Learning의 탄생 배경과 핵심 개념", expanded=True):
            st.markdown("""
            #### 🌱 탄생 배경
            - **문제**: 딥러닝 모델 학습에는 막대한 데이터와 컴퓨팅 자원 필요
            - **해결책**: 이미 학습된 지식을 재활용하자!
            - **영감**: 인간의 학습 방식 (자전거 → 오토바이 운전)

            #### 🎯 핵심 개념
            **Transfer Learning = 지식 전이 학습**
            ```
            Source Domain (원천 도메인) → Target Domain (목표 도메인)
            ImageNet 1000개 클래스    →    개/고양이 2개 클래스
            ```

            #### 📊 작동 원리
            1. **Low-level Features** (하위 레이어)
               - Edge, Corner, Texture 등 일반적 특징
               - 대부분의 이미지 작업에 공통적으로 유용
               - 보통 동결(Freeze)하여 재사용

            2. **High-level Features** (상위 레이어)
               - 클래스별 특화된 특징
               - Task-specific하므로 재학습 필요
               - Fine-tuning의 주요 대상

            #### 🚀 왜 효과적인가?
            - **Feature Hierarchy**: CNN은 계층적 특징 학습
            - **Universal Features**: 하위 레이어는 범용적
            - **Data Efficiency**: 적은 데이터로도 학습 가능
            - **Convergence Speed**: 빠른 수렴
            """)

        # Transfer Learning 방법론
        with st.expander("### 2. Transfer Learning 방법론 상세"):
            st.markdown("""
            #### 🔧 방법 1: Feature Extraction (특징 추출)
            ```python
            # 모든 레이어 동결
            for param in model.parameters():
                param.requires_grad = False

            # 마지막 레이어만 교체
            model.fc = nn.Linear(2048, num_classes)
            ```
            - **장점**: 빠름, 과적합 위험 낮음
            - **단점**: 성능 향상 제한적
            - **적용**: 데이터 매우 적을 때 (< 1000개)

            #### 🎨 방법 2: Fine-tuning (미세 조정)
            ```python
            # 초기 레이어만 동결
            for layer in model.layers[:-5]:
                layer.requires_grad = False

            # 상위 레이어는 학습 가능
            for layer in model.layers[-5:]:
                layer.requires_grad = True
            ```
            - **장점**: 높은 성능 달성 가능
            - **단점**: 과적합 위험, 학습 시간 증가
            - **적용**: 충분한 데이터 (> 5000개)

            #### 🔄 방법 3: Progressive Fine-tuning
            ```python
            # Step 1: 마지막 레이어만
            train_last_layer(epochs=10)

            # Step 2: 점진적으로 더 많은 레이어
            unfreeze_layers(n=2)
            train_model(epochs=5, lr=lr/10)

            # Step 3: 전체 미세조정
            unfreeze_all()
            train_model(epochs=3, lr=lr/100)
            ```
            - **장점**: 안정적 학습, 최고 성능
            - **단점**: 복잡한 구현, 시간 소요
            - **적용**: 중요한 프로젝트

            #### 📊 방법 선택 가이드
            | 데이터 양 | 유사도 | 추천 방법 |
            |----------|--------|----------|
            | 적음 + 높음 | Feature Extraction |
            | 적음 + 낮음 | Fine-tuning (상위 레이어) |
            | 많음 + 높음 | Fine-tuning (전체) |
            | 많음 + 낮음 | 처음부터 학습 or Progressive |
            """)

        # 실제 구현 예제
        with st.expander("### 3. 실제 구현 코드 예제"):
            st.markdown("""
            #### 🐕 예제: 개 품종 분류기 만들기

            ```python
            import torch
            import torch.nn as nn
            import torchvision.models as models
            from torch.optim import Adam
            from torch.optim.lr_scheduler import StepLR

            class DogBreedClassifier:
                def __init__(self, num_breeds=120):
                    # 1. 사전학습 모델 로드
                    self.model = models.resnet50(pretrained=True)

                    # 2. Feature Extraction 설정
                    for param in self.model.parameters():
                        param.requires_grad = False

                    # 3. 새로운 분류기 추가
                    num_features = self.model.fc.in_features
                    self.model.fc = nn.Sequential(
                        nn.Linear(num_features, 512),
                        nn.ReLU(),
                        nn.Dropout(0.3),
                        nn.Linear(512, num_breeds)
                    )

                def progressive_unfreeze(self, stage):
                    '''점진적 언프리징'''
                    if stage == 1:  # 마지막 블록만
                        for param in self.model.layer4.parameters():
                            param.requires_grad = True
                    elif stage == 2:  # 마지막 2개 블록
                        for param in self.model.layer3.parameters():
                            param.requires_grad = True
                    elif stage == 3:  # 전체
                        for param in self.model.parameters():
                            param.requires_grad = True

                def train_stage(self, dataloader, stage, epochs):
                    self.progressive_unfreeze(stage)

                    # 학습률 조정 (언프리징할수록 낮게)
                    lr = 1e-3 * (0.1 ** (stage - 1))
                    optimizer = Adam(
                        filter(lambda p: p.requires_grad,
                               self.model.parameters()),
                        lr=lr
                    )

                    for epoch in range(epochs):
                        # 학습 코드
                        pass

            # 사용 예시
            classifier = DogBreedClassifier(num_breeds=120)

            # Stage 1: Feature Extraction (10 epochs)
            classifier.train_stage(dataloader, stage=0, epochs=10)

            # Stage 2: Fine-tune 마지막 블록 (5 epochs)
            classifier.train_stage(dataloader, stage=1, epochs=5)

            # Stage 3: Fine-tune 더 많은 레이어 (3 epochs)
            classifier.train_stage(dataloader, stage=2, epochs=3)
            ```
            """)

        # 고급 기법
        with st.expander("### 4. Transfer Learning 고급 기법"):
            st.markdown("""
            #### 🎭 Domain Adaptation (도메인 적응)
            - **문제**: Source와 Target 도메인이 너무 다름
            - **해결**: Adversarial Training, MMD 등 활용
            ```python
            # Domain Adversarial Neural Network (DANN)
            class DANN(nn.Module):
                def __init__(self):
                    self.feature_extractor = ResNet50()
                    self.label_classifier = LabelClassifier()
                    self.domain_classifier = DomainClassifier()
                    self.gradient_reversal = GradientReversal()
            ```

            #### 🎯 Few-shot Learning (퓨샷 러닝)
            - **Prototypical Networks**: 클래스별 프로토타입 학습
            - **Siamese Networks**: 유사도 학습
            - **MAML**: 빠른 적응을 위한 메타 학습

            #### 🔄 Knowledge Distillation (지식 증류)
            ```python
            # Teacher-Student 모델
            def distillation_loss(student_output, teacher_output,
                                 true_labels, T=3, alpha=0.7):
                # Soft targets from teacher
                soft_loss = KL_div(
                    F.log_softmax(student_output/T),
                    F.softmax(teacher_output/T)
                ) * T * T

                # Hard targets
                hard_loss = F.cross_entropy(student_output, true_labels)

                return alpha * soft_loss + (1-alpha) * hard_loss
            ```

            #### 📊 Multi-task Learning (멀티태스크 러닝)
            - **Hard Parameter Sharing**: 레이어 공유
            - **Soft Parameter Sharing**: 정규화로 유사성 유도
            - **Cross-stitch Networks**: 태스크 간 정보 교환
            """)

    def _render_clip_theory(self):
        """CLIP 상세 이론"""
        st.markdown("## 🤖 CLIP (Contrastive Language-Image Pre-training) 이론과 응용")

        with st.expander("### 1. CLIP의 개발 배경과 핵심 아이디어", expanded=True):
            st.markdown("""
            #### 🌟 CLIP의 특징
            - **2021년 OpenAI 발표**: 비전-언어 모델의 패러다임 전환
            - **핵심**: 4억 개의 (이미지, 텍스트) 쌍으로 학습
            - **결과**: Zero-shot으로 ImageNet 정확도 76.2% 달성

            #### 🎯 핵심 아이디어: Contrastive Learning
            ```
            목표: 매칭되는 (이미지, 텍스트) 쌍은 가깝게
                 매칭되지 않는 쌍은 멀게

            [고양이 이미지] ←→ "귀여운 고양이" ✅ (가깝게)
            [고양이 이미지] ←→ "빨간 자동차" ❌ (멀게)
            ```

            #### 🏗️ CLIP 아키텍처
            ```
            이미지 → Image Encoder → 이미지 임베딩 (512차원)
                                          ↓
                                    코사인 유사도
                                          ↑
            텍스트 → Text Encoder → 텍스트 임베딩 (512차원)
            ```

            #### 📊 학습 과정
            1. **배치 구성**: N개의 (이미지, 텍스트) 쌍
            2. **인코딩**: 각각을 512차원 벡터로 변환
            3. **유사도 계산**: N×N 유사도 매트릭스
            4. **대조 학습**: 대각선은 1, 나머지는 0이 되도록
            """)

        with st.expander("### 2. CLIP 손실 함수와 학습 메커니즘"):
            st.markdown("""
            #### 📐 InfoNCE Loss (Contrastive Loss)
            ```python
            def clip_loss(image_embeddings, text_embeddings, temperature=0.07):
                # 정규화
                image_embeddings = F.normalize(image_embeddings)
                text_embeddings = F.normalize(text_embeddings)

                # 코사인 유사도 계산
                logits = image_embeddings @ text_embeddings.T / temperature

                # 대각선이 정답 (positive pairs)
                labels = torch.arange(len(logits))

                # 양방향 손실
                loss_i2t = F.cross_entropy(logits, labels)
                loss_t2i = F.cross_entropy(logits.T, labels)

                return (loss_i2t + loss_t2i) / 2
            ```

            #### 🌡️ Temperature Parameter
            - **역할**: 유사도 분포의 sharpness 조절
            - **낮은 온도 (0.01)**: 더 확실한 구분
            - **높은 온도 (0.1)**: 부드러운 구분
            - **CLIP 기본값**: 0.07

            #### 📊 학습 전략
            1. **Large Batch Size**: 32,768 (매우 큼)
               - 더 많은 negative samples
               - 안정적인 대조 학습

            2. **Mixed Precision Training**
               ```python
               with torch.cuda.amp.autocast():
                   image_features = image_encoder(images)
                   text_features = text_encoder(texts)
                   loss = clip_loss(image_features, text_features)
               ```

            3. **Gradient Accumulation**
               - 메모리 제약 극복
               - 효과적인 large batch 시뮬레이션
            """)

        with st.expander("### 3. CLIP의 Zero-shot 능력"):
            st.markdown("""
            #### 🎯 Zero-shot Classification
            ```python
            def zero_shot_classifier(image, class_names, model):
                # 1. 텍스트 프롬프트 생성
                text_prompts = [f"a photo of a {name}" for name in class_names]

                # 2. 인코딩
                image_features = model.encode_image(image)
                text_features = model.encode_text(text_prompts)

                # 3. 유사도 계산
                similarities = (image_features @ text_features.T)

                # 4. 소프트맥스로 확률 변환
                probs = similarities.softmax(dim=-1)

                return class_names[probs.argmax()]
            ```

            #### 🔄 Prompt Engineering for CLIP
            ```python
            # 기본 프롬프트
            "a photo of a {class}"

            # 개선된 프롬프트들
            templates = [
                "a photo of a {class}",
                "a bad photo of a {class}",
                "a origami {class}",
                "a photo of the large {class}",
                "a {class} in a video game",
                "art of a {class}",
                "a photo of the small {class}"
            ]

            # 앙상블로 성능 향상
            def ensemble_classify(image, class_name, templates):
                scores = []
                for template in templates:
                    text = template.format(class=class_name)
                    score = compute_similarity(image, text)
                    scores.append(score)
                return np.mean(scores)
            ```

            #### 📊 Zero-shot vs Fine-tuned 성능
            | Dataset | Zero-shot CLIP | Fine-tuned ResNet50 |
            |---------|---------------|-------------------|
            | ImageNet | 76.2% | 76.3% |
            | CIFAR-100 | 65.1% | 71.5% |
            | Food101 | 88.9% | 72.3% |
            | Flowers102 | 68.7% | 91.3% |
            """)

        with st.expander("### 4. CLIP 응용과 확장"):
            st.markdown("""
            #### 🎨 CLIP 기반 응용
            1. **DALL-E 2**: CLIP 임베딩 → 이미지 생성
            2. **CLIP-Seg**: 이미지 분할
            3. **CLIP4Clip**: 비디오 검색
            4. **AudioCLIP**: 오디오-비전-언어 통합

            #### 🔧 CLIP Fine-tuning 전략
            ```python
            class CLIPFineTuner:
                def __init__(self, clip_model):
                    self.clip = clip_model
                    # LoRA: Low-Rank Adaptation
                    self.lora_image = LoRAAdapter(self.clip.visual)
                    self.lora_text = LoRAAdapter(self.clip.transformer)

                def forward(self, images, texts):
                    # 원본 + LoRA 어댑터
                    image_features = self.clip.visual(images)
                    image_features += self.lora_image(images)

                    text_features = self.clip.transformer(texts)
                    text_features += self.lora_text(texts)

                    return image_features, text_features
            ```

            #### 🌐 다국어 CLIP
            - **mCLIP**: 다국어 지원
            - **XLM-R**: 100개 언어 텍스트 인코더
            - **Korean CLIP**: 한국어 특화 모델
            """)

    def _render_mathematical_foundation(self):
        """수학적 기초"""
        st.markdown("## 🔬 수학적 기초와 이론")

        with st.expander("### 1. Gradient 기반 최적화"):
            st.markdown("""
            #### 📐 Transfer Learning의 수학

            **목적 함수**:
            $$L_{total} = L_{task} + \\lambda L_{regularization}$$

            **Fine-tuning 그래디언트**:
            $$\\theta_{new} = \\theta_{pretrained} - \\eta \\nabla L_{task}$$

            여기서:
            - $\\theta_{pretrained}$: 사전학습 가중치
            - $\\eta$: 학습률 (보통 매우 작은 값)
            - $L_{task}$: 새로운 태스크 손실
            """)

        with st.expander("### 2. Contrastive Learning 수학"):
            st.markdown("""
            #### 📏 CLIP의 InfoNCE Loss

            $$L = -\\log\\frac{\\exp(sim(x_i, y_i)/\\tau)}{\\sum_{j=1}^{N} \\exp(sim(x_i, y_j)/\\tau)}$$

            여기서:
            - $sim(x, y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||}$ (코사인 유사도)
            - $\\tau$: temperature parameter
            - $N$: 배치 크기

            #### 🎯 최적화 목표
            - Positive pairs: $sim(x_i, y_i) \\rightarrow 1$
            - Negative pairs: $sim(x_i, y_j) \\rightarrow 0$ (i ≠ j)
            """)

    def _render_practical_guide(self):
        """실전 가이드"""
        st.markdown("## 💡 실전 가이드와 베스트 프랙티스")

        with st.expander("### 1. Transfer Learning 체크리스트", expanded=True):
            st.markdown("""
            #### ✅ 프로젝트 시작 전 체크리스트

            - [ ] **데이터 분석**
              - 데이터셋 크기: _____개
              - 클래스 수: _____개
              - 클래스 불균형 여부: Yes/No
              - Source 도메인과 유사도: High/Medium/Low

            - [ ] **모델 선택**
              - 정확도 우선: ResNet, EfficientNet
              - 속도 우선: MobileNet, ShuffleNet
              - 균형: EfficientNet-B0~B3

            - [ ] **학습 전략**
              - 데이터 < 1000: Feature Extraction
              - 1000 < 데이터 < 10000: Partial Fine-tuning
              - 데이터 > 10000: Full Fine-tuning

            - [ ] **하이퍼파라미터**
              - 초기 학습률: 1e-4 ~ 1e-3
              - 배치 크기: 최대한 크게 (메모리 허용 범위)
              - 에폭: Early Stopping 사용
            """)

        with st.expander("### 2. CLIP 활용 가이드"):
            st.markdown("""
            #### 🎯 CLIP 활용 시나리오

            **1. 제로샷 이미지 분류**
            ```python
            # 신규 클래스 추가 시 재학습 불필요
            new_classes = ["전기차", "하이브리드차", "수소차"]
            predictions = clip_classify(image, new_classes)
            ```

            **2. 이미지 검색 시스템**
            ```python
            # 자연어로 이미지 검색
            query = "일몰 때 해변에서 서핑하는 사람"
            results = clip_search(query, image_database)
            ```

            **3. 콘텐츠 모더레이션**
            ```python
            # 부적절한 콘텐츠 필터링
            inappropriate_prompts = ["violence", "adult content", ...]
            scores = clip_score(image, inappropriate_prompts)
            ```

            **4. 멀티모달 추천 시스템**
            ```python
            # 이미지 + 텍스트 기반 추천
            user_preference = "미니멀한 북유럽 스타일"
            recommendations = clip_recommend(products, user_preference)
            ```
            """)

        with st.expander("### 3. 트러블슈팅 가이드"):
            st.markdown("""
            #### 🔧 일반적인 문제와 해결책

            | 문제 | 원인 | 해결책 |
            |------|------|--------|
            | 과적합 | 데이터 부족 | Data Augmentation, Dropout 증가 |
            | 수렴 안 됨 | 학습률 너무 큼 | 학습률 감소 (10배) |
            | 성능 저하 | Catastrophic Forgetting | Lower learning rate, Regularization |
            | 메모리 부족 | 배치 크기 너무 큼 | Gradient Accumulation |
            | 느린 학습 | 너무 많은 레이어 학습 | Feature Extraction 먼저 |

            #### 💊 Quick Fixes
            ```python
            # 과적합 해결
            model.add_module('dropout', nn.Dropout(0.5))

            # 학습 불안정 해결
            optimizer = torch.optim.Adam(model.parameters(),
                                        lr=1e-4, weight_decay=1e-5)
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, patience=3, factor=0.5)

            # 메모리 최적화
            with torch.cuda.amp.autocast():
                output = model(input)
                loss = criterion(output, target)
            ```
            """)

        # 이전 간단한 이론 내용도 유지
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("📌 Quick Reference")
            st.markdown("""
            **Transfer Learning 핵심**
            - 사전학습 → 전이 → 미세조정
            - 적은 데이터로 높은 성능

            **CLIP 핵심**
            - 이미지-텍스트 통합 임베딩
            - Zero-shot 분류 가능
            """)

        with col2:
            st.subheader("3. Multi-modal Learning")
            st.markdown("""
            - **CLIP**: 텍스트-이미지 연결
            - **DALL-E**: 텍스트로 이미지 생성
            - **Flamingo**: 비전-언어 이해
            - **ALIGN**: 대규모 비전-언어 모델
            """)

            st.subheader("4. 실제 활용 사례")
            st.markdown("""
            - **의료 AI**: X-ray, MRI 분석
            - **자율주행**: 객체 인식 및 추적
            - **품질 검사**: 제조업 불량 검출
            - **콘텐츠 검색**: 이미지-텍스트 검색
            """)

    def _render_transfer_learning_tab(self):
        """Transfer Learning 탭"""
        st.header("🔄 Transfer Learning 실습")

        # 탭 생성: 예제 학습과 사용자 이미지 분석
        sub_tabs = st.tabs(["📚 예제로 학습하기", "🔧 내 모델 Fine-tuning하기"])

        with sub_tabs[0]:
            st.markdown("### 1. 사전 훈련 모델 선택")

            col1, col2, col3 = st.columns(3)

            with col1:
                model_name = st.selectbox(
                    "모델 선택",
                    ["ResNet50", "VGG16", "EfficientNet", "MobileNet", "DenseNet"],
                    key="model_select_example"
                )

            with col2:
                pretrained = st.checkbox("사전 훈련 가중치 사용", value=True, key="pretrained_example")

            with col3:
                num_classes = st.number_input("출력 클래스 수", min_value=2, value=10, key="num_classes_example")

            # 모델 정보 표시
            if st.button("모델 정보 보기", key="model_info_example"):
                self._show_model_info(model_name)

            st.markdown("### 2. Transfer Learning 방법")

            method = st.radio(
                "학습 방법 선택",
                ["Feature Extraction (빠름)", "Fine-tuning (정확함)", "전체 학습 (느림)"],
                key="method_example"
            )

            # 코드 예시
            with st.expander("📝 코드 보기"):
                code = self.transfer_helper.get_transfer_learning_code(model_name, num_classes, method)
                st.code(code, language="python")

        with sub_tabs[1]:
            st.markdown("### 🔧 커스텀 데이터셋으로 Fine-tuning")

            # 베이스 모델 설명
            with st.expander("📚 베이스 모델이란?", expanded=True):
                st.markdown("""
                **베이스 모델(Base Model)**: ImageNet 등 대규모 데이터셋으로 사전 학습된 모델

                #### 🎯 Fine-tuning 개념
                - **정의**: 사전 학습된 모델을 새로운 데이터셋에 맞게 재학습하는 과정
                - **장점**:
                  - 적은 데이터로도 높은 성능 달성
                  - 학습 시간 단축 (수일 → 수시간)
                  - 이미 학습된 특징(feature) 활용

                #### 📊 주요 베이스 모델 비교
                | 모델 | 파라미터 | 정확도 | 속도 | 용도 |
                |------|---------|--------|------|------|
                | **ResNet50** | 25.6M | 92.1% | 중간 | 범용, 안정적 |
                | **EfficientNet-B0** | 5.3M | 93.3% | 빠름 | 효율적, 모바일 |
                | **MobileNetV2** | 3.5M | 90.1% | 매우 빠름 | 경량, 실시간 |

                #### 🔧 Fine-tuning 프로세스
                1. **베이스 모델 로드**: 사전 학습된 가중치 불러오기
                2. **마지막 레이어 교체**: 새로운 클래스 수에 맞게 변경
                3. **선택적 동결**: 일부 레이어는 고정, 일부만 학습
                4. **학습**: 커스텀 데이터로 재학습
                5. **평가**: 성능 검증 및 최적화

                #### 📈 성능 향상 확인 방법
                - **Fine-tuning 전**: 일반 ImageNet 모델 → 70-80% 정확도
                - **Fine-tuning 후**: 커스텀 데이터 학습 → 90-95% 정확도
                - **평가 지표**: Accuracy, Precision, Recall, F1-Score, Confusion Matrix
                """)

            st.markdown("---")

            # 파일 업로드
            uploaded_files = st.file_uploader(
                "학습할 이미지 업로드 (클래스별로 폴더 구분)",
                type=['png', 'jpg', 'jpeg'],
                accept_multiple_files=True,
                key="custom_dataset"
            )

            if uploaded_files:
                col1, col2 = st.columns(2)

                with col1:
                    model_choice = st.selectbox(
                        "베이스 모델 선택",
                        ["ResNet50", "EfficientNet-B0", "MobileNetV2"],
                        key="model_custom",
                        help="ResNet50: 가장 안정적, EfficientNet: 높은 정확도, MobileNet: 빠른 속도"
                    )

                    learning_rate = st.slider(
                        "학습률",
                        min_value=0.0001,
                        max_value=0.01,
                        value=0.001,
                        format="%.4f",
                        key="lr_custom"
                    )

                with col2:
                    epochs = st.slider("에폭 수", min_value=1, max_value=50, value=10, key="epochs_custom")
                    batch_size = st.select_slider("배치 크기", options=[8, 16, 32, 64], value=32, key="batch_custom")

                # 선택된 모델 상세 정보
                with st.expander(f"🔍 {model_choice} 상세 정보"):
                    if model_choice == "ResNet50":
                        st.markdown("""
                        **ResNet50 (Residual Network)**
                        - **개발**: Microsoft Research (2015)
                        - **특징**: Skip Connection으로 깊은 네트워크 학습 가능
                        - **구조**: 50개 레이어, Bottleneck 블록
                        - **장점**: 안정적 학습, 범용적 사용
                        - **단점**: 모델 크기가 큼 (98MB)
                        - **적합한 경우**: 정확도가 중요한 경우, 충분한 컴퓨팅 자원
                        """)
                    elif model_choice == "EfficientNet-B0":
                        st.markdown("""
                        **EfficientNet-B0**
                        - **개발**: Google Brain (2019)
                        - **특징**: Compound Scaling으로 효율적 확장
                        - **구조**: MBConv 블록, Squeeze-and-Excitation
                        - **장점**: 적은 파라미터로 높은 성능
                        - **단점**: 학습 시 불안정할 수 있음
                        - **적합한 경우**: 효율성과 정확도 모두 중요한 경우
                        """)
                    else:  # MobileNetV2
                        st.markdown("""
                        **MobileNetV2**
                        - **개발**: Google (2018)
                        - **특징**: Inverted Residual Block, Linear Bottleneck
                        - **구조**: Depthwise Separable Convolution
                        - **장점**: 매우 가벼움 (14MB), 빠른 추론
                        - **단점**: 정확도가 상대적으로 낮음
                        - **적합한 경우**: 모바일/엣지 디바이스, 실시간 처리
                        """)

                if st.button("🚀 Fine-tuning 시작", key="start_finetuning"):
                    with st.spinner(f"{model_choice}을 베이스로 모델을 학습하는 중..."):
                        # 실제 fine-tuning 로직은 여기에 구현
                        st.info(f"🏗️ 베이스 모델 {model_choice} 로드 중...")
                        progress_bar = st.progress(0)

                        status = st.empty()
                        for i in range(epochs):
                            progress_bar.progress((i + 1) / epochs)
                            status.text(f"Epoch {i+1}/{epochs} - Loss: {0.5 - i*0.02:.3f}")

                        st.success(f"✅ Fine-tuning 완료! {model_choice} 기반 커스텀 모델 생성됨")

                        # 성능 비교 섹션
                        st.markdown("---")
                        st.markdown("### 📊 Fine-tuning 성능 비교")

                        col1, col2, col3 = st.columns(3)

                        # 시뮬레이션된 성능 지표
                        with col1:
                            st.metric(
                                label="Fine-tuning 전 정확도",
                                value="72.3%",
                                delta=None,
                                help="ImageNet 가중치 그대로 사용"
                            )

                        with col2:
                            st.metric(
                                label="Fine-tuning 후 정확도",
                                value="94.7%",
                                delta="+22.4%",
                                delta_color="normal",
                                help="커스텀 데이터로 재학습"
                            )

                        with col3:
                            st.metric(
                                label="성능 향상률",
                                value="31.0%",
                                delta="개선됨",
                                help="(94.7-72.3)/72.3 * 100"
                            )

                        # 학습 곡선 그래프
                        with st.expander("📈 학습 곡선 및 성능 분석", expanded=True):
                            fig = self.transfer_helper.plot_learning_curves()
                            st.pyplot(fig)

                            st.markdown("""
                            #### 🎯 성능 향상 확인 방법

                            **1. 정확도 (Accuracy) 비교**
                            - Fine-tuning 전: 사전학습 모델 그대로 → 낮은 정확도
                            - Fine-tuning 후: 커스텀 데이터 학습 → 높은 정확도

                            **2. 손실 함수 (Loss) 추적**
                            - Training Loss: 학습 데이터에서의 오차
                            - Validation Loss: 검증 데이터에서의 오차
                            - 두 값이 모두 감소하면 성능 향상

                            **3. 혼동 행렬 (Confusion Matrix)**
                            - 각 클래스별 예측 정확도 확인
                            - 오분류 패턴 분석
                            """)

                        # 혼동 행렬
                        with st.expander("🔍 상세 성능 분석"):
                            tab1, tab2, tab3 = st.tabs(["혼동 행렬", "클래스별 성능", "특징 공간"])

                            with tab1:
                                fig_cm = self.transfer_helper.create_confusion_matrix(5)
                                st.pyplot(fig_cm)
                                st.caption("Fine-tuning 후 혼동 행렬 - 대각선이 진할수록 좋은 성능")

                            with tab2:
                                # 클래스별 성능 메트릭
                                st.markdown("""
                                | 클래스 | Precision | Recall | F1-Score |
                                |--------|-----------|--------|----------|
                                | Class 0 | 0.95 | 0.93 | 0.94 |
                                | Class 1 | 0.92 | 0.96 | 0.94 |
                                | Class 2 | 0.96 | 0.94 | 0.95 |
                                | Class 3 | 0.94 | 0.95 | 0.94 |
                                | Class 4 | 0.97 | 0.95 | 0.96 |
                                """)

                            with tab3:
                                fig_tsne = self.transfer_helper.visualize_feature_space()
                                st.pyplot(fig_tsne)
                                st.caption("t-SNE로 시각화한 특징 공간 - 클래스가 잘 분리될수록 좋음")

                        # 실전 팁
                        st.info("""
                        💡 **Fine-tuning 성능 향상 팁**
                        - Early Stopping: Validation loss가 증가하기 시작하면 학습 중단
                        - Learning Rate Scheduling: 학습률을 점진적으로 감소
                        - Data Augmentation: 데이터 증강으로 과적합 방지
                        - Regularization: Dropout, Weight Decay 적용
                        """)

    def _render_clip_search_tab(self):
        """CLIP Image Search 탭"""
        st.header("🖼️ CLIP을 사용한 이미지 검색")

        # CLIP 설명
        with st.expander("💡 CLIP 이미지 검색이란?", expanded=True):
            st.markdown("""
            **CLIP (Contrastive Language-Image Pre-training)**: OpenAI가 개발한 멀티모달 AI 모델

            #### 🎯 텍스트 → 이미지 검색 기능
            - **개념**: 자연어 텍스트로 이미지를 검색하는 혁신적인 기술
            - **작동 원리**:
              1. 텍스트를 벡터로 변환 (텍스트 임베딩)
              2. 이미지를 동일한 벡터 공간으로 변환 (이미지 임베딩)
              3. 벡터 간 유사도 계산 (코사인 유사도)
              4. 가장 유사한 이미지 반환

            #### 🚀 기존 검색과의 차이점
            | 기존 검색 | CLIP 검색 |
            |----------|----------|
            | 태그/메타데이터 기반 | 이미지 내용 직접 이해 |
            | 정확한 키워드 필요 | 자연스러운 문장 가능 |
            | 미리 라벨링 필요 | 라벨링 불필요 |
            | 제한적 검색 | 창의적 검색 가능 |

            #### 📋 활용 예시
            - **전자상거래**: "파란색 스트라이프 셔츠" → 관련 제품 이미지
            - **갤러리**: "일몰이 있는 해변 풍경" → 관련 사진 검색
            - **의료**: "폐에 결절이 있는 X-ray" → 유사 의료 이미지
            - **SNS**: "귀여운 강아지가 공놀이 하는 모습" → 관련 게시물
            """)

        # 탭 생성
        sub_tabs = st.tabs(["🔍 텍스트로 검색", "🖼️ 이미지로 검색", "📊 임베딩 시각화"])

        with sub_tabs[0]:
            st.markdown("### 텍스트 → 이미지 검색")

            # 사용 방법 안내
            st.info("""
            📝 **사용 방법**
            1. 아래에 검색하고 싶은 내용을 텍스트로 입력
            2. 검색 대상이 될 이미지들을 업로드
            3. CLIP이 텍스트와 가장 유사한 이미지를 찾아줍니다

            **예시 검색어**:
            - "빨간 자동차"
            - "웃고 있는 사람"
            - "푸른 하늘과 흰 구름"
            - "커피 한 잔"
            - "노트북으로 일하는 사람"
            """)

            search_query = st.text_input(
                "검색할 텍스트 입력",
                placeholder="예: 빨간 자동차, 행복한 강아지, 일몰 해변",
                key="clip_text_search",
                help="자연스러운 문장으로 입력해도 됩니다"
            )

            # 이미지 데이터베이스
            uploaded_images = st.file_uploader(
                "검색할 이미지 데이터베이스 업로드",
                type=['png', 'jpg', 'jpeg'],
                accept_multiple_files=True,
                key="clip_db_text",
                help="여러 개의 이미지를 업로드하면 그 중에서 검색합니다"
            )

            if search_query and uploaded_images:
                if st.button("🔍 CLIP 검색 실행", key="run_clip_text"):
                    with st.spinner("CLIP 모델로 검색 중..."):
                        # CLIP 검색 시뮬레이션
                        st.success(f"✅ '{search_query}'와 가장 유사한 이미지를 찾았습니다!")

                        # 검색 과정 설명
                        with st.expander("🔬 CLIP 검색 과정", expanded=False):
                            st.markdown(f"""
                            1. **텍스트 인코딩**: "{search_query}" → 512차원 벡터
                            2. **이미지 인코딩**: {len(uploaded_images)}개 이미지 → 각각 512차원 벡터
                            3. **유사도 계산**: 코사인 유사도로 텍스트-이미지 매칭
                            4. **순위 결정**: 유사도가 높은 순으로 정렬
                            """)

                        st.markdown("### 🏆 검색 결과 (상위 3개)")

                        # 결과 표시 (시뮬레이션)
                        cols = st.columns(3)
                        similarities = [np.random.uniform(0.7, 0.95) for _ in range(3)]
                        similarities.sort(reverse=True)

                        for i, img_file in enumerate(uploaded_images[:3]):
                            if i < 3:
                                img = Image.open(img_file)
                                with cols[i]:
                                    st.image(img, use_column_width=True)
                                    st.metric(
                                        label=f"#{i+1} 순위",
                                        value=f"{similarities[i]:.1%}",
                                        delta="유사도",
                                        help=f"텍스트 '{search_query}'와의 의미적 유사도"
                                    )

                        # 결과 해석
                        st.info("""
                        💡 **유사도 해석**
                        - 90% 이상: 매우 높은 일치
                        - 80-90%: 높은 관련성
                        - 70-80%: 중간 관련성
                        - 70% 미만: 낮은 관련성
                        """)

        with sub_tabs[1]:
            st.markdown("### 이미지 → 이미지 검색")

            query_image = st.file_uploader(
                "쿼리 이미지 업로드",
                type=['png', 'jpg', 'jpeg'],
                key="clip_query_image"
            )

            db_images = st.file_uploader(
                "검색할 이미지 데이터베이스",
                type=['png', 'jpg', 'jpeg'],
                accept_multiple_files=True,
                key="clip_db_image"
            )

            if query_image and db_images:
                col1, col2 = st.columns([1, 2])

                with col1:
                    st.image(query_image, caption="쿼리 이미지")

                with col2:
                    if st.button("🔍 유사 이미지 검색", key="run_clip_image"):
                        st.info("유사한 이미지를 검색 중...")

        with sub_tabs[2]:
            st.markdown("### 📊 CLIP 임베딩 시각화")

            if st.button("임베딩 공간 시각화", key="visualize_embeddings"):
                # 임베딩 시각화 (시뮬레이션)
                fig = self.multimodal_helper.visualize_clip_embeddings()
                st.pyplot(fig)

    def _render_api_comparison_tab(self):
        """Multi-modal API 비교 탭"""
        st.header("🔍 Multi-modal API 비교 분석")

        # 2025년 9월 기준 API 정보 표시
        with st.expander("📅 2025년 9월 기준 API 접근 방법", expanded=True):
            st.markdown("""
            ### 🔗 OpenAI CLIP
            - **접근 방식**: 오픈소스 다운로드 (API 서비스 아님)
            - **설치**: `pip install git+https://github.com/openai/CLIP.git`
            - **특징**: API 키 불필요, 완전 무료, 로컬 실행
            - **응답 속도**: <100ms (GPU 사용 시)

            ### 🤖 Google Gemini API (2025년 권장)
            - **Vision API 대체**: Gemini가 Vision API를 대체하는 추세
            - **Google AI Studio 접근 방법**:
              1. ai.google.dev 접속
              2. Google 계정 로그인
              3. "Get API key" 클릭
              4. "Create API key in new project" 선택
              5. API 키 생성 (형식: AIza...)
            - **무료 할당량**: 분당 60건, 신용카드 불필요
            - **강점**: 멀티모달 처리, PDF 직접 처리, 90분 비디오 지원

            ### 🤗 Hugging Face API
            - **토큰 생성**: HuggingFace.co → Settings → Access Tokens → New Token
            - **토큰 형식**: `hf_xxxxx`
            - **2025년 권장**: Fine-grained 토큰, 앱별 별도 토큰 생성
            """)

        st.markdown("---")

        # API 선택
        selected_apis = st.multiselect(
            "비교할 API 선택",
            ["OpenAI CLIP", "Google Vision API", "Azure Computer Vision",
             "AWS Rekognition", "Hugging Face", "OpenAI GPT-4V"],
            default=["OpenAI CLIP", "Google Vision API", "Hugging Face"],
            key="api_comparison"
        )

        if len(selected_apis) >= 2:
            # 비교 차트 생성
            st.subheader("📊 API 기능 비교")

            comparison_df = self.multimodal_helper.get_api_comparison_data(selected_apis)
            st.dataframe(comparison_df, use_container_width=True)

            # 성능 벤치마크
            st.subheader("⚡ 성능 벤치마크")

            col1, col2 = st.columns(2)

            with col1:
                # 속도 비교 차트
                fig_speed = self.multimodal_helper.create_speed_comparison_chart(selected_apis)
                st.pyplot(fig_speed)

            with col2:
                # 정확도 비교 차트
                fig_accuracy = self.multimodal_helper.create_accuracy_comparison_chart(selected_apis)
                st.pyplot(fig_accuracy)

            # 사용 사례별 추천
            st.subheader("💡 사용 사례별 추천")

            use_case = st.selectbox(
                "사용 사례 선택",
                ["이미지 검색", "콘텐츠 모더레이션", "의료 이미지 분석",
                 "제품 추천", "자동 태깅", "시각적 질의응답"],
                key="use_case"
            )

            recommendation = self.multimodal_helper.get_api_recommendation(use_case, selected_apis)
            st.info(recommendation)

    def _render_feature_extraction_tab(self):
        """특징 추출 탭"""
        st.header("🎨 특징 추출 및 시각화")

        uploaded_file = st.file_uploader(
            "이미지 업로드",
            type=['png', 'jpg', 'jpeg'],
            key="feature_extraction"
        )

        if uploaded_file:
            image = Image.open(uploaded_file)

            col1, col2 = st.columns([1, 1])

            with col1:
                st.image(image, caption="원본 이미지")

                model_choice = st.selectbox(
                    "특징 추출 모델",
                    ["ResNet50", "VGG16", "EfficientNet", "CLIP"],
                    key="feature_model"
                )

                layer_choice = st.selectbox(
                    "추출할 레이어",
                    ["Early layers", "Middle layers", "Late layers", "Final layer"],
                    key="feature_layer"
                )

            with col2:
                if st.button("🎨 특징 추출", key="extract_features"):
                    with st.spinner("특징을 추출하는 중..."):
                        # 특징 추출 시각화 (시뮬레이션)
                        fig = self.transfer_helper.visualize_features(image, model_choice, layer_choice)
                        st.pyplot(fig)

            # 특징 맵 분석
            if st.checkbox("상세 분석 보기", key="detailed_analysis"):
                st.subheader("📊 특징 맵 상세 분석")

                tabs = st.tabs(["히트맵", "3D 시각화", "통계"])

                with tabs[0]:
                    st.info("특징 맵 히트맵 시각화")
                    # 히트맵 시각화 코드

                with tabs[1]:
                    st.info("3D 특징 공간 시각화")
                    # 3D 시각화 코드

                with tabs[2]:
                    st.info("특징 통계 분석")
                    # 통계 분석 코드

    def _render_integrated_analysis_tab(self):
        """통합 분석 탭"""
        st.header("📊 Transfer Learning 통합 분석")

        analysis_type = st.selectbox(
            "분석 유형 선택",
            ["모델 성능 비교", "학습 곡선 분석", "혼동 행렬", "특징 공간 분석"],
            key="integrated_analysis"
        )

        if analysis_type == "모델 성능 비교":
            st.subheader("🏆 모델 성능 비교")

            # 모델 선택
            models = st.multiselect(
                "비교할 모델",
                ["ResNet50", "VGG16", "EfficientNet", "MobileNet", "DenseNet"],
                default=["ResNet50", "EfficientNet"],
                key="model_comparison"
            )

            if len(models) >= 2:
                # 성능 메트릭 표시
                metrics_df = self.transfer_helper.get_model_metrics(models)
                st.dataframe(metrics_df, use_container_width=True)

                # 차트 생성
                fig = self.transfer_helper.create_performance_chart(models)
                st.pyplot(fig)

        elif analysis_type == "학습 곡선 분석":
            st.subheader("📈 학습 곡선 분석")

            # 학습 곡선 시각화
            fig = self.transfer_helper.plot_learning_curves()
            st.pyplot(fig)

            # 분석 인사이트
            st.info("""
            **학습 곡선 해석**:
            - 훈련 손실과 검증 손실의 차이가 크면 과적합
            - 두 곡선이 모두 높으면 과소적합
            - 최적점은 검증 손실이 최소인 지점
            """)

        elif analysis_type == "혼동 행렬":
            st.subheader("🔢 혼동 행렬 분석")

            # 클래스 수 선택
            num_classes = st.slider("클래스 수", min_value=2, max_value=10, value=5, key="confusion_classes")

            # 혼동 행렬 생성 및 표시
            fig = self.transfer_helper.create_confusion_matrix(num_classes)
            st.pyplot(fig)

        else:  # 특징 공간 분석
            st.subheader("🌌 특징 공간 분석")

            # t-SNE 시각화
            fig = self.transfer_helper.visualize_feature_space()
            st.pyplot(fig)

    def _render_project_tab(self):
        """실전 프로젝트 탭"""
        st.header("🚀 실전 Transfer Learning 프로젝트")

        project_type = st.selectbox(
            "프로젝트 선택",
            ["🏥 의료 이미지 분류", "🏭 제조업 품질 검사", "🎨 스타일 전이", "🔍 상품 검색 시스템"],
            key="project_type"
        )

        if project_type == "🏥 의료 이미지 분류":
            self._render_medical_project()
        elif project_type == "🏭 제조업 품질 검사":
            self._render_quality_control_project()
        elif project_type == "🎨 스타일 전이":
            self._render_style_transfer_project()
        else:
            self._render_product_search_project()

    def _render_medical_project(self):
        """의료 이미지 분류 프로젝트"""
        st.subheader("🏥 X-ray 이미지 분류 시스템")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **프로젝트 목표**:
            - 흉부 X-ray에서 폐렴 검출
            - Transfer Learning으로 정확도 향상
            - 적은 데이터로 높은 성능 달성
            """)

            uploaded_xray = st.file_uploader(
                "X-ray 이미지 업로드",
                type=['png', 'jpg', 'jpeg'],
                key="xray_upload"
            )

        with col2:
            if uploaded_xray:
                st.image(uploaded_xray, caption="업로드된 X-ray")

                if st.button("🔍 진단 시작", key="diagnose"):
                    with st.spinner("AI 분석 중..."):
                        # 진단 시뮬레이션
                        st.success("분석 완료!")
                        st.metric("정상 확률", "15%")
                        st.metric("폐렴 확률", "85%", delta="주의 필요")

    def _render_quality_control_project(self):
        """제조업 품질 검사 프로젝트"""
        st.subheader("🏭 제품 불량 검출 시스템")

        st.markdown("""
        **시스템 특징**:
        - 실시간 불량품 검출
        - 다양한 불량 유형 분류
        - Transfer Learning으로 빠른 배포
        """)

        # 불량 유형 설정
        defect_types = st.multiselect(
            "검출할 불량 유형",
            ["스크래치", "찌그러짐", "변색", "크랙", "이물질"],
            default=["스크래치", "크랙"],
            key="defect_types"
        )

        if st.button("시스템 시작", key="start_qc"):
            st.info("품질 검사 시스템이 실행 중입니다...")

    def _render_style_transfer_project(self):
        """스타일 전이 프로젝트"""
        st.subheader("🎨 Neural Style Transfer")

        col1, col2 = st.columns(2)

        with col1:
            content_image = st.file_uploader(
                "콘텐츠 이미지",
                type=['png', 'jpg', 'jpeg'],
                key="content_img"
            )
            if content_image:
                st.image(content_image, caption="콘텐츠")

        with col2:
            style_image = st.file_uploader(
                "스타일 이미지",
                type=['png', 'jpg', 'jpeg'],
                key="style_img"
            )
            if style_image:
                st.image(style_image, caption="스타일")

        if content_image and style_image:
            style_weight = st.slider("스타일 강도", 0.0, 1.0, 0.5, key="style_weight")

            if st.button("🎨 스타일 전이 시작", key="transfer_style"):
                with st.spinner("스타일을 전이하는 중..."):
                    st.info("Neural Style Transfer 처리 중...")
                    st.success("스타일 전이 완료!")

    def _render_product_search_project(self):
        """상품 검색 시스템 프로젝트"""
        st.subheader("🔍 시각적 상품 검색 시스템")

        search_method = st.radio(
            "검색 방법",
            ["텍스트로 검색", "이미지로 검색", "하이브리드 검색"],
            key="search_method"
        )

        if search_method == "텍스트로 검색":
            query = st.text_input("검색어 입력", placeholder="빨간 운동화", key="text_query")
        elif search_method == "이미지로 검색":
            query_img = st.file_uploader("참조 이미지", type=['png', 'jpg', 'jpeg'], key="img_query")
        else:
            col1, col2 = st.columns(2)
            with col1:
                text_q = st.text_input("텍스트", placeholder="편안한", key="hybrid_text")
            with col2:
                img_q = st.file_uploader("이미지", type=['png', 'jpg', 'jpeg'], key="hybrid_img")

        if st.button("🔍 검색", key="search_products"):
            st.success("유사한 상품을 찾았습니다!")
            # 검색 결과 표시

    def _show_model_info(self, model_name):
        """모델 정보 표시"""
        model_info = {
            "ResNet50": {
                "parameters": "25.6M",
                "layers": "50",
                "year": "2015",
                "accuracy": "92.1%"
            },
            "VGG16": {
                "parameters": "138M",
                "layers": "16",
                "year": "2014",
                "accuracy": "90.1%"
            },
            "EfficientNet": {
                "parameters": "5.3M",
                "layers": "Variable",
                "year": "2019",
                "accuracy": "91.7%"
            },
            "MobileNet": {
                "parameters": "4.2M",
                "layers": "28",
                "year": "2017",
                "accuracy": "89.5%"
            },
            "DenseNet": {
                "parameters": "25.6M",
                "layers": "121",
                "year": "2016",
                "accuracy": "91.8%"
            }
        }

        if model_name in model_info:
            info = model_info[model_name]
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("Parameters", info["parameters"])
            col2.metric("Layers", info["layers"])
            col3.metric("Year", info["year"])
            col4.metric("ImageNet Top-5", info["accuracy"])