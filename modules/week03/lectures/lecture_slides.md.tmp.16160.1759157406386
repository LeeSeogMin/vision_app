# Week 3: 전이학습 + 멀티모달 API 활용

## 강의 슬라이드

---

# 📚 3주차 학습 목표

## 오늘 배울 내용

1. **Transfer Learning의 원리와 실전**
2. **Vision-Language Models 이해**
3. **멀티모달 API 활용법**
4. **자연어 기반 사진첩 검색 앱 구축**

---

# Part 1: Transfer Learning

## 🎯 전이학습이란?

### 정의
> "이미 학습된 지식을 새로운 문제에 적용하는 기법"

### 왜 필요한가?
- **데이터 부족**: 적은 데이터로도 높은 성능
- **시간 절약**: 처음부터 학습 불필요
- **비용 절감**: GPU 사용 최소화
- **성능 향상**: 더 나은 초기 가중치

---

## 🔄 Transfer Learning 과정

```
[ImageNet 1000 Classes]
         ↓
    [Pretrained Model]
         ↓
    [Remove Last Layer]
         ↓
    [Add Custom Layers]
         ↓
    [Your Task: 10 Classes]
```

### 핵심 아이디어
- 하위 레이어: 일반적 특징 (edges, textures)
- 상위 레이어: 태스크 특화 특징

---

## 📊 Feature Extraction vs Fine-tuning

### Feature Extraction
```python
# 모든 레이어 동결
for param in model.parameters():
    param.requires_grad = False

# 마지막 레이어만 교체
model.fc = nn.Linear(features, num_classes)
```

**장점**: 빠름, 적은 데이터
**단점**: 성능 한계

### Fine-tuning
```python
# 일부/전체 레이어 학습
for param in model.parameters():
    param.requires_grad = True
```

**장점**: 최고 성능
**단점**: 오버피팅 위험

---

## 💡 언제 무엇을 선택할까?

### Decision Tree

```
데이터 양?
├─ 적음 (<1000)
│  └─ Feature Extraction
└─ 많음 (>10000)
   └─ 유사도?
      ├─ 높음
      │  └─ Feature Extraction
      └─ 낮음
         └─ Fine-tuning
```

---

## 🚀 실전 코드: Transfer Learning

```python
import torch
import torchvision.models as models

# 1. 사전훈련 모델 로드
model = models.resnet50(pretrained=True)

# 2. Feature Extraction 설정
for param in model.parameters():
    param.requires_grad = False

# 3. 새로운 분류기 추가
num_features = model.fc.in_features
model.fc = nn.Sequential(
    nn.Linear(num_features, 256),
    nn.ReLU(),
    nn.Dropout(0.5),
    nn.Linear(256, num_classes)
)

# 4. 학습
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)
```

---

# Part 2: Vision-Language Models

## 🌟 CLIP의 혁신

### Contrastive Language-Image Pre-training

```
   Text Encoder          Image Encoder
        ↓                      ↓
   Text Embedding       Image Embedding
        ↓                      ↓
        └──── Similarity ─────┘
```

### 핵심 특징
- **4억 개** 이미지-텍스트 쌍으로 학습
- **Zero-shot** 이미지 분류 가능
- **자연어**로 이미지 검색

---

## 🔍 CLIP 작동 원리

### Contrastive Learning

```python
# 유사도 행렬
similarities = image_embeddings @ text_embeddings.T

# 대각선: 매칭 쌍 (positive)
# 나머지: 비매칭 쌍 (negative)

loss = cross_entropy(similarities, labels)
```

### 목표
- Positive pairs: 유사도 ↑
- Negative pairs: 유사도 ↓

---

## 📐 임베딩 공간

### Shared Embedding Space

```
    "a photo of a cat"  →  [0.2, 0.8, ...]
              ↓
         Similarity
              ↑
    [Cat Image] →  [0.21, 0.79, ...]
```

### 응용
- 이미지 검색
- Zero-shot 분류
- 이미지 생성 가이드

---

## 🎨 CLIP 활용 예제

```python
from transformers import CLIPProcessor, CLIPModel

# 모델 로드
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# 이미지-텍스트 매칭
image = Image.open("photo.jpg")
texts = ["a cat", "a dog", "a bird"]

inputs = processor(text=texts, images=image, 
                  return_tensors="pt", padding=True)
outputs = model(**inputs)

# 유사도 계산
logits_per_image = outputs.logits_per_image
probs = logits_per_image.softmax(dim=1)
```

---

# Part 3: 멀티모달 API 활용

## 📅 2025년 9월 기준 API 접근 방법

### 1. 🔗 OpenAI CLIP
- **접근 방식**: 오픈소스 다운로드 (API 서비스 아님)
- **설치 방법**:
  ```bash
  # GitHub 직접 설치
  pip install git+https://github.com/openai/CLIP.git
  # 또는 Hugging Face 버전
  pip install transformers
  ```
- **특징**: API 키 불필요, 로컬 실행, 완전 무료
- **응답 속도**: <100ms (GPU 사용 시)

### 2. 🤖 Google Gemini API (권장)
- **2025년 현재**: Vision API를 대체하는 추세
- **강점**:
  - 복잡한 이미지 이해 및 추론
  - 멀티모달 처리 (텍스트, 이미지, 비디오)
  - PDF 직접 처리 (OCR 불필요)
  - 90분까지 비디오 지원
- **접근 방법**:
  1. Google Cloud Console 접속
  2. 프로젝트 생성 → Gemini API 활성화
  3. API 키 생성
- **무료 할당량**: 충분한 무료 티어 제공

### 3. 🤗 Hugging Face API
- **토큰 생성 방법**:
  1. HuggingFace.co 계정 생성
  2. Settings → Access Tokens
  3. "New Token" 클릭
  4. 권한 설정 (read/write)
  5. 토큰 생성 (형식: `hf_xxxxx`)
- **2025년 권장사항**:
  - 프로덕션에는 fine-grained 토큰 사용
  - 앱별로 별도 토큰 생성

---

## 🔧 Gemini API 실습

```python
import google.generativeai as genai

# API 설정
genai.configure(api_key="YOUR_API_KEY")
model = genai.GenerativeModel('gemini-1.5-flash')

# 이미지 분석
image = Image.open("photo.jpg")
response = model.generate_content([
    "Describe this image in detail",
    image
])

print(response.text)
```

### 활용 예시
- 이미지 캡션 생성
- Visual Q&A
- OCR 및 텍스트 추출

---

## 🤖 Together AI 활용

```python
import together
import base64

# API 설정
together.api_key = "YOUR_API_KEY"

# 이미지 인코딩
with open("photo.jpg", "rb") as f:
    image_base64 = base64.b64encode(f.read()).decode()

# 분석 요청
response = together.Complete.create(
    model="meta-llama/Llama-3.2-11B-Vision",
    prompt=f"<image>{image_base64}</image>\nDescribe this image",
    max_tokens=256
)

print(response['output']['choices'][0]['text'])
```

---

## 📊 API 성능 비교

### 벤치마크 결과

| API | 응답 시간 | 정확도 | 비용 |
|-----|---------|-------|------|
| Gemini | 1.2s | 95% | Free tier |
| Llama Vision | 2.1s | 92% | Free credits |
| CLIP | 0.1s | 88% | 100% Free |

### 선택 가이드
- **속도 중요**: CLIP
- **정확도 중요**: Gemini
- **커스터마이징**: Llama Vision

---

# Part 4: 통합 프로젝트

## 🎯 자연어 기반 사진첩 검색 앱

### 시스템 아키텍처

```
User Query → CLIP Encoder → Similarity Search
                ↓
           Image Results
                ↓
          Gemini API → Captions
                ↓
           Final Display
```

### 핵심 기능
1. 텍스트로 사진 검색
2. 유사 이미지 찾기
3. 자동 캡션 생성
4. 고급 필터링

---

## 💻 통합 구현

```python
class SmartPhotoAlbum:
    def __init__(self):
        # Transfer Learning 모델
        self.classifier = load_transfer_model()
        
        # CLIP 모델
        self.clip = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        
        # Gemini API
        self.gemini = genai.GenerativeModel('gemini-1.5-flash')
    
    def search(self, query):
        # CLIP으로 검색
        results = self.clip_search(query)
        
        # Gemini로 캡션 추가
        for result in results:
            result['caption'] = self.generate_caption(result['image'])
        
        return results
```

---

## 🚀 Gradio 인터페이스

```python
import gradio as gr

def create_app():
    with gr.Blocks() as app:
        gr.Markdown("# 🖼️ Smart Photo Album")
        
        with gr.Tab("Search"):
            query = gr.Textbox(label="Search photos")
            results = gr.Gallery(label="Results")
            
            query.submit(search_photos, inputs=[query], 
                        outputs=[results])
        
        with gr.Tab("Upload"):
            upload = gr.File(label="Upload photos")
            status = gr.Textbox(label="Status")
            
            upload.change(process_upload, inputs=[upload], 
                         outputs=[status])
    
    return app

app = create_app()
app.launch()
```

---

## 📈 성능 최적화

### 1. 임베딩 캐싱
```python
# 사전 계산 및 저장
embeddings = compute_embeddings(images)
np.save('embeddings.npy', embeddings)

# 빠른 로드
embeddings = np.load('embeddings.npy')
```

### 2. 배치 처리
```python
# 한 번에 여러 이미지 처리
batch_size = 32
for i in range(0, len(images), batch_size):
    batch = images[i:i+batch_size]
    process_batch(batch)
```

### 3. 비동기 처리
```python
import asyncio

async def process_async(images):
    tasks = [process_image(img) for img in images]
    results = await asyncio.gather(*tasks)
    return results
```

---

# 실습 시간

## 🧪 Lab 3: 통합 실습

### Step 1: Transfer Learning
1. ResNet50으로 분류기 만들기
2. Feature Extraction vs Fine-tuning 비교

### Step 2: CLIP 검색
1. 이미지 인덱싱
2. 텍스트 검색 구현

### Step 3: API 통합
1. Gemini로 캡션 생성
2. 성능 비교

### Step 4: 앱 배포
1. Gradio 인터페이스
2. Hugging Face Space 배포

---

## 🎯 핵심 정리

### 오늘 배운 내용
✅ Transfer Learning으로 효율적인 모델 구축
✅ CLIP으로 텍스트-이미지 검색
✅ 멀티모달 API 활용법
✅ 통합 시스템 구현

### 다음 주 예고
- Vision Transformer (ViT)
- Self-Attention 메커니즘
- DINO와 자기지도학습

---

## 📝 과제 안내

### Assignment 3: 멀티모달 검색 시스템

**요구사항**:
1. 최소 3가지 검색 모드
2. 2개 이상 API 통합
3. 성능 비교 리포트
4. HF Space 배포

**평가 기준**:
- 기능성 40%
- 성능 20%
- UI/UX 20%
- 코드 품질 10%
- 창의성 10%

**제출 기한**: 다음 주 수업 전

---

## 💬 Q&A

### 자주 묻는 질문

**Q1: Transfer Learning vs Scratch?**
- 거의 항상 Transfer Learning이 유리
- 특수한 도메인만 예외

**Q2: CLIP vs 일반 분류기?**
- CLIP: 유연성, Zero-shot
- 분류기: 특정 태스크 최적화

**Q3: API 비용은?**
- 수업용으로는 모두 무료
- 상업용 사용 시 과금

---

## 🔗 참고 자료

### 논문
- [CLIP Paper](https://arxiv.org/abs/2103.00020)
- [Vision Transformers](https://arxiv.org/abs/2010.11929)

### 튜토리얼
- [PyTorch Transfer Learning](https://pytorch.org/tutorials/)
- [Hugging Face CLIP Guide](https://huggingface.co/docs/transformers/model_doc/clip)

### 코드
- [Week 3 GitHub](https://github.com/course/week03)
- [예제 노트북](https://colab.research.google.com/)

---

# Thank You! 🙏

## 다음 주에 만나요!

### 연락처
- 이메일: newmind68@hs.ac.kr
- 오피스 아워: 수요일 14:00-16:00

### 온라인 리소스
- 강의 자료: [Course Website]
- 질문: [Course Forum]

---